{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optim\n",
    "参考：[module-torch.optim](https://pytorch.org/docs/stable/optim.html)\n",
    "\n",
    "简单明了解释：包含不同的优化算法更新参数的软件包。接口通用，还可以集成更复杂算法（自定义）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing it\n",
    "要构造一个Optimizer，必须为其提供一个包含参数的可迭代项（所有参数都应为Variables）以进行优化。然后，可以指定优化器特定的选项，例如学习率，权重衰减等。\n",
    "\n",
    "tips：使用`.cuda()`时，需要保持位置一致。\n",
    "\n",
    "例如\n",
    "```python\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = optim.Adam([var1, var2], lr=0.0001)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# per-parameter options\n",
    "`optimizer`支持指定参数，通过传入可迭代的`dict`，且包含一个`params`key，对应的需要优化的参数列表。\n",
    "\n",
    "ps:同样可以用关键字指定\n",
    "\n",
    "```python\n",
    "optim.SGD([\n",
    "    {'params': model.base.parameters()},\n",
    "    {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
    "    ], lr=1e-2, momentum=0.9)\n",
    "```\n",
    "\n",
    "`model.base`的参数以默认的`1e-2`，而`model.classifier`参数以学习率`1e-3`优化。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking an optimization step\n",
    "所有的优化器都实现了一个方法:`.step()`，当`loss.backward()`被调用后立即调用以更新参数。\n",
    "\n",
    "包括了2个调用形式\n",
    "\n",
    "## 简单版本：optimizer.step()\n",
    "```python\n",
    "for input, target in dataset:\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input)\n",
    "    loss = loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "## 闭包版本：optimizer.step(closure)\n",
    "一些优化算法（例如共轭梯度和LBFGS）需要多次重新评估函数，因此必须传递一个闭包，以使它们可以重新计算模型。闭合应清除梯度，计算loss，然后将其返回。\n",
    "\n",
    "```python\n",
    "for input, target in dataset:\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    optimizer.step(closure)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化算法\n",
    "中文参考：[torch.optim](https://pytorch.apachecn.org/docs/1.4/87.html)\\\n",
    "英文参考：[optim](https://pytorch.org/docs/stable/optim.html)\n",
    "\n",
    "参考知乎：[PyTorch 学习笔记（七）：PyTorch的十个优化器](https://zhuanlan.zhihu.com/p/64885176)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adadelta\n",
    "\n",
    "## Adagrad\n",
    "\n",
    "## Adam\n",
    "\n",
    "## AdamW\n",
    "\n",
    "## SparseAdam\n",
    "\n",
    "## Adamax\n",
    "\n",
    "## ASGD\n",
    "\n",
    "## LBFGS\n",
    "\n",
    "## RMSprop\n",
    "\n",
    "## Rprop\n",
    "该优化方法适用于full-batch，不适用于mini-batch，因而在min-batch大行其道的时代里，很少见到。\n",
    "\n",
    "## SGD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调整学习率(torch.optim.lr_scheduler包)\n",
    "`torch.optim.lr_scheduler`提供了几种epochs级别的学习率调整方法。\n",
    "\n",
    "[torch.optim.lr_scheduler.ReduceLROnPlateau](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.ReduceLROnPlateau)允许<font color=red>基于某些验证指标来动态的减少学习率</font>\n",
    "\n",
    "使用方式:\n",
    "```python\n",
    ">>> scheduler = ...\n",
    ">>> for epoch in range(100):\n",
    ">>>     train(...)\n",
    ">>>     validate(...)\n",
    ">>>     scheduler.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LambdaLR\n",
    "使用示例\n",
    "\n",
    "```python\n",
    ">>> # Assuming optimizer has two groups.\n",
    ">>> lambda1 = lambda epoch: epoch // 30\n",
    ">>> lambda2 = lambda epoch: 0.95 ** epoch\n",
    ">>> scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])\n",
    ">>> for epoch in range(100):\n",
    ">>>     train(...)\n",
    ">>>     validate(...)\n",
    ">>>     scheduler.step()\n",
    "```\n",
    "\n",
    "## MultiplicativeLR\n",
    "\n",
    "将每个参数组的学习率乘以指定函数中给定的因子。 当last_epoch = -1时，将初始lr设置为lr。\n",
    "\n",
    "```python\n",
    ">>> lmbda = lambda epoch: 0.95\n",
    ">>> scheduler = MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
    ">>> for epoch in range(100):\n",
    ">>>     train(...)\n",
    ">>>     validate(...)\n",
    ">>>     scheduler.step()\n",
    "```\n",
    "\n",
    "## StepLR\n",
    "\n",
    "在每个step_size时期，通过γ降低每个参数组的学习率。 注意，这种衰减可能与此调度程序外部的学习速率的其他更改同时发生。 当last_epoch = -1时，将初始lr设置为lr。\n",
    "\n",
    "```python\n",
    ">>> # Assuming optimizer uses lr = 0.05 for all groups\n",
    ">>> # lr = 0.05     if epoch < 30\n",
    ">>> # lr = 0.005    if 30 <= epoch < 60\n",
    ">>> # lr = 0.0005   if 60 <= epoch < 90\n",
    ">>> # ...\n",
    ">>> scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    ">>> for epoch in range(100):\n",
    ">>>     train(...)\n",
    ">>>     validate(...)\n",
    ">>>     scheduler.step()\n",
    "```\n",
    "\n",
    "## MultiStepLR\n",
    "一旦epoch达到设定的milestones，则gamma进行衰减。\n",
    "```python\n",
    ">>> # Assuming optimizer uses lr = 0.05 for all groups\n",
    ">>> # lr = 0.05     if epoch < 30\n",
    ">>> # lr = 0.005    if 30 <= epoch < 80\n",
    ">>> # lr = 0.0005   if epoch >= 80\n",
    ">>> scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
    ">>> for epoch in range(100):\n",
    ">>>     train(...)\n",
    ">>>     validate(...)\n",
    ">>>     scheduler.step()\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "## ExponentialLR\n",
    "```python\n",
    "optimizer (Optimizer) – Wrapped optimizer.\n",
    "gamma (float) – Multiplicative factor of learning rate decay.\n",
    "last_epoch (int) – The index of last epoch. Default: -1.\n",
    "verbose (bool) – If True, prints a message to stdout for each update. Default: False.\n",
    "```\n",
    "\n",
    "在每个epoch，衰减每个参数组的学习率。\n",
    "\n",
    "\n",
    "## CosineAnnealingLR\n",
    "\n",
    "复杂的consine函数。参见 [torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=-1, verbose=False)](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.CosineAnnealingLR)\n",
    "\n",
    "## ReduceLROnPlateau\n",
    "当指标停止改善时，降低学习率。 一旦学习停滞，模型通常会受益于将学习率降低2-10倍。 该调度程序读取一个指标数量，如果在“patience(耐心)”时期没有看到改善，则学习速度会降低。\n",
    "\n",
    "```python\n",
    ">>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    ">>> scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    ">>> for epoch in range(10):\n",
    ">>>     train(...)\n",
    ">>>     val_loss = validate(...)\n",
    ">>>     # Note that step should be called after validate()\n",
    ">>>     scheduler.step(val_loss)\n",
    "```\n",
    "\n",
    "## CyclicLR\n",
    "带边界的学习率调整策略,比较复杂，@TODO：[torch.optim.lr_scheduler.CyclicLR](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.CyclicLR)\n",
    "\n",
    "```python\n",
    ">>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    ">>> scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1)\n",
    ">>> data_loader = torch.utils.data.DataLoader(...)\n",
    ">>> for epoch in range(10):\n",
    ">>>     for batch in data_loader:\n",
    ">>>         train_batch(...)\n",
    ">>>         scheduler.step()\n",
    "```\n",
    "\n",
    "## OneCycleLR\n",
    "\n",
    "参考《Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates》\n",
    "\n",
    "@TODO\n",
    "\n",
    "\n",
    "## CosineAnnealingWarmRestarts\n",
    "\n",
    "consine算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Weight Averaging算法\n",
    "@TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
