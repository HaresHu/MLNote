{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch实现NS的Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as DataUtil\n",
    "import collections\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文件读取与预处理\n",
    "参考：\n",
    "\n",
    "[word2vec CPP代码](https://github.com/tmikolov/word2vec/blob/master/word2vec.c)\n",
    "\n",
    "[wor2vec.ipynb](https://localhost:8888/notebooks/mm_note.git/0010-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%B8%9A%E5%8A%A1%E7%AE%97%E6%B3%95/word2vec.ipynb)\n",
    "\n",
    "## 读取并生成词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文件已经是切词过的，读取数据,去掉\\ufeff字节顺序\n",
    "# 按行读取\n",
    "data_path = \"../../../learnMLData/三国演义_cut.txt\"\n",
    "text_toks = []\n",
    "with open(data_path, encoding='UTF-8-sig') as fp:\n",
    "    for line in fp.readlines():\n",
    "        toks = line.strip().lower().split()\n",
    "        text_toks.append(toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 过滤低频词建立索引\n",
    "以下部分可以套用到其他应用：如商品推荐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 低频词过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤额掉低频词\n",
    "mini_filter = 5\n",
    "vocab_dict =  dict(filter(lambda kv:kv[1] > mini_filter, Counter([tok for toks in text_toks for tok in toks]).most_common()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = list(vocab_dict.keys())\n",
    "word2idx = {word: idx for idx, word in enumerate(idx2word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词按索引\n",
    "text_toks2idx_data = [\n",
    "    [word2idx[tok] for tok in raw_toks if tok in vocab_dict] for raw_toks in text_toks\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [278, 2333, 1976, 1703, 318, 2621, 75, 922, 432, 3310, 1704],\n",
       " [1506, 1292, 432, 3026]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_toks2idx_data[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下采样\n",
    "解决稀有词和常见词之间的不平衡问题，使用了一种简单的下采样方法：训练集中的每个词$w_i$被丢弃，其概率由公式计算,实际计算做了修正,$f(w_i)$为词的频率,t为阈值，通常是$1e-5$\n",
    "\n",
    "$$\n",
    "P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}}\n",
    "$$实际计算做了修正\n",
    "c代码\n",
    "\n",
    "```c\n",
    "real ran = (sqrt(vocab[word].cn / (sample * train_words)) + 1) * (sample * train_words) / vocab[word].cn;\n",
    "next_random = next_random * (unsigned long long)25214903917 + 11;\n",
    "if (ran < (next_random & 0xFFFF) / (real)65536) continue;\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = sum(len(raw_toks) for raw_toks in text_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_sample(idx):\n",
    "    word_freq = vocab_dict[idx2word[idx]] / train_words\n",
    "    ran = 1 - math.sqrt(1e-5 / word_freq)\n",
    "    #print(ran, random.uniform(0, 1), vocab_dict[idx2word[idx]])\n",
    "    return random.uniform(0, 1) < ran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对数据集下采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_text_toks_idx = [[tok for tok in raw_data if not sub_sample(tok)] for raw_data in text_toks2idx_data] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[], [278, 2333, 1976, 1703, 318, 2621, 75, 922, 432, 3310, 1704]],\n",
       " [[], [3310, 1704]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_toks2idx_data[0:2], subsampled_text_toks_idx[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现Dataset\n",
    "按顺序读取token，并返回这个中心词token与相关的上下文词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中心词与context词抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按行生成上下文\n",
    "def get_context_list(raw_data, window):\n",
    "    # 此时，raw_data 长度最少为2\n",
    "    context_list = []\n",
    "    for i in range(len(raw_data)):\n",
    "        start_i = max(0, i - window)\n",
    "        end_i = min(i + 1 + window, len(raw_data))\n",
    "        index_list = list(range(start_i, end_i))\n",
    "        index_list.remove(i)\n",
    "        context_list.append([raw_data[j] for j in index_list])\n",
    "    # print(\"context_list=\", context_list)\n",
    "    return context_list\n",
    "\n",
    "# 生成一一映射的数据(中心词， [上下文])\n",
    "def gen_cc_data(dataset, window):\n",
    "    center_list, context_list = [], []\n",
    "    for raw_data in dataset:\n",
    "        if len(raw_data) < 2:\n",
    "            continue\n",
    "        center_list += raw_data\n",
    "        context_list.extend(get_context_list(raw_data, window))\n",
    "    return center_list, context_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tiny_dataset = [list(range(7)), list(range(7, 10))]\n",
    "#tiny_dataset\n",
    "#gen_cc_data(tiny_dataset, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_window = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_list, context_list = gen_cc_data(subsampled_text_toks_idx, g_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative sample：必须采集K个\n",
    "词频的0.75次方占比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_freq = np.array([vocab_dict[word] ** 0.75 for word in idx2word], dtype=np.float32)\n",
    "weighted_freq = weighted_freq / np.sum(weighted_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word_index = list(range(len(idx2word)))\n",
    "idx2word_index[0:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00817728042602539\n"
     ]
    }
   ],
   "source": [
    "start_t = time.time()\n",
    "np.random.choice(idx2word_index, 5, False, weighted_freq)\n",
    "print(time.time() - start_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "五万数据抽样耗时36m，后续优化：减少抽样，一次性抽足够多数据（确保内存不爆）\n",
    "\n",
    "以下负采样用于CBOW模型，所以每次只需要抽取k个就可以（训练时是输入的平均），skipgram则需要抽取与context上下文相同量（误差平均，每个context也是一个正样例）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_ns_data(word2idx, vocab_dict, center_list, context_list, k = 5):\n",
    "    weighted_freq = np.array([vocab_dict[word] ** 0.75 for word in idx2word], dtype=np.float32)\n",
    "    weighted_freq = weighted_freq / np.sum(weighted_freq)\n",
    "    \n",
    "    idx2word_index = list(range(len(idx2word)))\n",
    "    ret = []\n",
    "    for i in range(len(center_list)):\n",
    "        # 不放回抽样\n",
    "        # 防止抽样数据全部都是context, k*2\n",
    "        # 最小抽样数\n",
    "        #min_sample_num = min(len(context_list[i]), k)\n",
    "        ns_data = []\n",
    "        ns_pre = np.random.choice(idx2word_index, k * 4, False, weighted_freq)\n",
    "        for ns in ns_pre:\n",
    "            if ns in ns_data:\n",
    "                continue\n",
    "            #if len(ns_data) < min_sample_num:\n",
    "            if len(ns_data) < k:\n",
    "                ns_data.append(ns)\n",
    "        ret.append(ns_data)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.43799901008606\n"
     ]
    }
   ],
   "source": [
    "start_t = time.time()\n",
    "ns_data = gen_ns_data(word2idx, vocab_dict, center_list, context_list, k = g_window)\n",
    "print(time.time() - start_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[905, 1450, 1023, 6245, 394],\n",
       " [2733, 2718, 3285, 974, 106],\n",
       " [176, 31, 552, 76, 641],\n",
       " [104, 704, 1884, 4349, 2906],\n",
       " [4470, 3256, 34, 2921, 1819],\n",
       " [481, 3255, 421, 90, 4928],\n",
       " [190, 2071, 138, 6094, 1503],\n",
       " [2216, 5742, 175, 35, 5154],\n",
       " [203, 6021, 4835, 210, 2742],\n",
       " [119, 371, 1086, 328, 1192]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns_data[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义Dataset与Loader算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法1：自定义简单dataset与DataLoader的collate_fn方法\n",
    "\n",
    "核心是将Dataset中`__getitem__`实现的复杂功能移植到collate_fn中，只是此函数输入是batch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法2：直接在自定义的dataset中实现\n",
    "将getitem中的功能移到`__getitem__(self, i)`中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_max_conpect = g_window * 2\n",
    "g_common_label = torch.tensor([1] + [0] * g_window)\n",
    "\n",
    "class W2VDataset2(torch.utils.data.Dataset):\n",
    "    def __init__(self, center_list, context_list, ns_list):\n",
    "\n",
    "        super(W2VDataset2, self).__init__()\n",
    "        # 三者长度必须一致\n",
    "        assert len(center_list) == len(context_list) == len(ns_list)\n",
    "        self.center_list = torch.LongTensor(center_list)\n",
    "        self.context_list = context_list\n",
    "        self.ns_list = ns_list\n",
    "           \n",
    "    def __len__(self):\n",
    "        return len(self.center_list)  \n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        center = self.center_list[i]\n",
    "        context = self.context_list[i]\n",
    "        negative = self.ns_list[i]\n",
    "        \n",
    "        real_context_len = len(context)\n",
    "        \n",
    "        context_list = context + [0] * (g_max_conpect - real_context_len)\n",
    "        \n",
    "        # 长度为负采样参数k=g_window\n",
    "        center_negative = [center] + negative\n",
    "        \n",
    "        label = g_common_label\n",
    "        \n",
    "        # 生成CBOW输出层index\n",
    "        center_negative_list = [center] + negative\n",
    "        return torch.LongTensor([center]), torch.LongTensor(context_list), torch.LongTensor(center_negative), real_context_len, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = W2VDataset2(center_list, context_list, ns_data)\n",
    "dataloader2 = torch.utils.data.DataLoader(dataset2, 32, shuffle=True, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3310]),\n",
       " tensor([1704,    0,    0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " tensor([3310,  905, 1450, 1023, 6245,  394]),\n",
       " 1,\n",
       " tensor([1, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0\n",
      "tensor([[ 714],\n",
      "        [2002],\n",
      "        [3687],\n",
      "        [ 765],\n",
      "        [ 841],\n",
      "        [5749],\n",
      "        [1423],\n",
      "        [5416],\n",
      "        [4703],\n",
      "        [1865],\n",
      "        [ 368],\n",
      "        [1538],\n",
      "        [1630],\n",
      "        [1672],\n",
      "        [3466],\n",
      "        [ 328],\n",
      "        [3827],\n",
      "        [3767],\n",
      "        [ 661],\n",
      "        [ 525],\n",
      "        [  40],\n",
      "        [6313],\n",
      "        [5029],\n",
      "        [2418],\n",
      "        [ 295],\n",
      "        [ 390],\n",
      "        [2769],\n",
      "        [1502],\n",
      "        [4034],\n",
      "        [  57],\n",
      "        [ 372],\n",
      "        [2862]])\n",
      "tensor([[5593, 2580,   64,  337, 6394,  983,  136,  983,  690, 1839],\n",
      "        [ 838, 2324,  231, 1187, 2317, 3439, 6275, 3802,    0,    0],\n",
      "        [3147, 5135, 1605, 1418,  601, 1573, 1418,  196, 5720,    8],\n",
      "        [   4,  904, 2276, 1929,   26,  682, 4406, 1542,   53, 1186],\n",
      "        [ 205,   56, 1342, 3745, 5537, 2169, 1800, 2824, 3513,  106],\n",
      "        [ 849,  560, 3406, 3404, 3404, 1312, 4507,  550, 3407,  385],\n",
      "        [1127, 6013, 1924, 1220, 1220, 2934, 1526,  670,  138, 3785],\n",
      "        [3817, 3782, 2175, 4415, 4826, 5965, 1115, 6308,  583, 1820],\n",
      "        [ 180,  216, 3160, 2502, 6342, 1152, 2603, 1196,  352, 1602],\n",
      "        [  31, 5271, 1561, 3613,  747, 2989,  100, 6377,  972, 4852],\n",
      "        [2675, 2879,  706, 2104, 2210, 1950, 1714, 1895, 2134,  753],\n",
      "        [3148, 4062,  330, 5604, 5088, 1479, 2538, 5555,  426, 2123],\n",
      "        [3592,  728, 4332, 1644, 3545, 4258,  606,   54, 1331,   55],\n",
      "        [4281, 6262,   91,   15, 3135,  841, 2291,  419, 2021, 1928],\n",
      "        [1161, 5678, 1453, 6295, 2556,  828, 4668, 2440,  919, 4774],\n",
      "        [3951,   16, 1616,  375,  779,   26, 1600,  744,  508,   53],\n",
      "        [5101, 3892, 1689, 5969,  884, 2431,  745, 5143, 4246, 1566],\n",
      "        [1939, 1718, 1673, 1309, 4477, 4576, 3094, 2056,   58, 5778],\n",
      "        [1631, 4575,  506,   12, 2578, 1005, 5339,  409, 3049, 2739],\n",
      "        [4435, 4236, 3092,    9,   34,    0,    0,    0,    0,    0],\n",
      "        [1608, 4566, 3873, 2138, 2615, 1182, 2600, 2063, 6352, 5430],\n",
      "        [ 106,  612,  965, 1721,   46,  956,   70, 1800,   15,    7],\n",
      "        [4022, 1638, 1547,   92,  226, 1547,  765,  807, 5898,  911],\n",
      "        [2754, 2758, 4783, 1427, 1295,   60,  215,    0,    0,    0],\n",
      "        [1682, 3018, 1130, 4704, 4225, 5418,    0,    0,    0,    0],\n",
      "        [3778, 1435, 2550,  837, 1692, 1539, 5880, 1241,  283, 1609],\n",
      "        [ 990,  316, 1509,   40, 3754,    4, 2191, 6161, 4369,    0],\n",
      "        [ 999,  101, 1961, 6017, 5522, 2885,  654, 1999,  871,    0],\n",
      "        [1050,    8,  478, 1797,  570, 5060, 5061, 4499,  364, 1672],\n",
      "        [1408, 1176, 1723, 5710, 1375,  676, 1164, 4390,  103, 4267],\n",
      "        [ 262, 3471, 2345, 3772, 5524,    0,    0,    0,    0,    0],\n",
      "        [  73,  207,   74, 1222, 1056, 2533, 1167, 2246,  943,  805]])\n",
      "tensor([[ 714, 2062,  721,  164,  163,    3],\n",
      "        [2002,  731, 5630, 1970, 4812,  641],\n",
      "        [3687, 3629, 3682, 5081, 5687,  202],\n",
      "        [ 765,  808, 6151,  218, 3322, 1009],\n",
      "        [ 841, 3268,  822, 5945, 2097, 3139],\n",
      "        [5749,   53,  854, 6109,  233,  148],\n",
      "        [1423,  122,   84,  677, 3241,  353],\n",
      "        [5416,    7,  257,   24, 4788, 4315],\n",
      "        [4703, 6240,  191,  534,  746, 1640],\n",
      "        [1865,  617, 1015, 4145, 5900,  611],\n",
      "        [ 368,   11,  116, 4250,  644,   15],\n",
      "        [1538, 5865,  238, 5755,  319, 1032],\n",
      "        [1630, 3932, 6336,  692, 4107, 1762],\n",
      "        [1672,   25,  141, 1216,  102, 6220],\n",
      "        [3466, 2946,  830,   52, 2517, 6175],\n",
      "        [ 328, 2370,  558,   45, 1810, 1547],\n",
      "        [3827, 3883,   41, 3615,  210, 6213],\n",
      "        [3767, 2091,  541,   52,  442, 3342],\n",
      "        [ 661,   25, 1044,  228, 3089, 1039],\n",
      "        [ 525,    0, 1607,  525, 4588, 1620],\n",
      "        [  40, 2326, 2188, 2688,  306,    0],\n",
      "        [6313,  124, 5237, 3217, 3818,  266],\n",
      "        [5029,  128,   53,  910, 5126, 4902],\n",
      "        [2418, 3057,  436, 1435,  334,  665],\n",
      "        [ 295, 1421,  159, 2182,  933, 2353],\n",
      "        [ 390,    0,  741, 2226,   83,  930],\n",
      "        [2769,  619,  229,  152, 1635,   53],\n",
      "        [1502, 1782, 5447, 3084,   33, 5975],\n",
      "        [4034,  249,  177,   10, 2417,  462],\n",
      "        [  57, 5421,   87, 4775,  174, 1646],\n",
      "        [ 372,  787, 3508, 5317,  533, 5837],\n",
      "        [2862, 2516, 3007,   41,    1, 4534]])\n",
      "tensor([10,  8, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10,  5, 10, 10, 10,  7,  6, 10,  9,  9, 10, 10,  5, 10])\n",
      "tensor([[1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "for i, (center_batch, context_batch, center_ns_batch, context_len_batch, label_batch) in enumerate(dataloader2):\n",
    "    if i == 1:\n",
    "        break\n",
    "    print(\"i=%d\" % (i))\n",
    "    print(center_batch)\n",
    "    print(context_batch)\n",
    "    print(center_ns_batch)\n",
    "    print(context_len_batch)\n",
    "    \n",
    "    print(label_batch)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个函数，去处理非空"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_embed = torch.nn.Embedding(len(vocab_dict), 3)\n",
    "\n",
    "out_embed = torch.nn.Embedding(len(vocab_dict), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_tensor = torch.tensor([[1],[2], [3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.6210,  1.7269,  1.4544]],\n",
       " \n",
       "         [[-2.0192, -0.0111,  0.1607]],\n",
       " \n",
       "         [[-0.5709,  1.1892,  1.3197]]], grad_fn=<EmbeddingBackward>),\n",
       " tensor([[-1.6210,  1.7269,  1.4544],\n",
       "         [-2.0192, -0.0111,  0.1607],\n",
       "         [-0.5709,  1.1892,  1.3197]], grad_fn=<EmbeddingBackward>),\n",
       " tensor([[[-1.6210],\n",
       "          [ 1.7269],\n",
       "          [ 1.4544]],\n",
       " \n",
       "         [[-2.0192],\n",
       "          [-0.0111],\n",
       "          [ 0.1607]],\n",
       " \n",
       "         [[-0.5709],\n",
       "          [ 1.1892],\n",
       "          [ 1.3197]]], grad_fn=<UnsqueezeBackward0>),\n",
       " torch.Size([3, 3, 1]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_embed(idx_tensor), in_embed(torch.tensor([1,2,3])),in_embed(torch.tensor([1,2,3])).unsqueeze(2),in_embed(torch.tensor([1,2,3])).unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "tensor([[[1, 2, 3, 4]],\n",
      "\n",
      "        [[5, 6, 7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3, 4],[5,6,7,8]])\n",
    "print(x) \n",
    "print(torch.unsqueeze(x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.6210,  1.7269,  1.4544],\n",
       "          [-2.0192, -0.0111,  0.1607],\n",
       "          [-0.5709,  1.1892,  1.3197]],\n",
       " \n",
       "         [[-1.0274,  1.0965,  1.3659],\n",
       "          [-0.0862,  0.1579, -0.1276],\n",
       "          [ 0.3759, -0.8672, -0.4328]]], grad_fn=<EmbeddingBackward>),\n",
       " tensor([[[-1.6210, -2.0192, -0.5709],\n",
       "          [ 1.7269, -0.0111,  1.1892],\n",
       "          [ 1.4544,  0.1607,  1.3197]],\n",
       " \n",
       "         [[-1.0274, -0.0862,  0.3759],\n",
       "          [ 1.0965,  0.1579, -0.8672],\n",
       "          [ 1.3659, -0.1276, -0.4328]]], grad_fn=<PermuteBackward>))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_embed(torch.tensor([[1,2,3], [7,19,36]])), in_embed(torch.tensor([[1,2,3], [7,19,36]])).permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_h2(in_embed, in_batch_idx, in_batch_sz):\n",
    "    h_list = []\n",
    "    for i in range(len(in_batch_idx)):\n",
    "        #print(i, in_batch_sz[i])\n",
    "        #print(in_embed(in_batch_idx[i][0: in_batch_sz[i]]))\n",
    "        \n",
    "        t = in_embed(in_batch_idx[i][0: in_batch_sz[i]]).sum(0) / in_batch_sz[i]\n",
    "        #print(\"sum=\",t)\n",
    "        #print(\"sum as numpy=\", t.detach().numpy())\n",
    "        h_list.append([t.detach().numpy().tolist()])\n",
    "    #print(\"h_list\",h_list)\n",
    "    return torch.tensor(h_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_h(in_embed, in_batch_idx, in_batch_sz):\n",
    "    h_list = None\n",
    "    for i in range(len(in_batch_idx)):\n",
    "        #print(i, in_batch_sz[i])\n",
    "        #print(in_embed(in_batch_idx[i][0: in_batch_sz[i]]))\n",
    "        \n",
    "        t = in_embed(in_batch_idx[i][0: in_batch_sz[i]]).sum(0) / in_batch_sz[i]\n",
    "        #print(\"t1=\",t)\n",
    "        t = torch.unsqueeze(t, 0)\n",
    "        t = torch.unsqueeze(t, 0)\n",
    "        #print(\"t2=\",t)\n",
    "        if h_list is None:\n",
    "            h_list = t\n",
    "        else:\n",
    "            h_list = torch.cat((h_list, t), 0)\n",
    "            #print(\"h_list=\", h_list)\n",
    "    return h_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0\n",
      "tensor([[[ 0.1500,  0.2027,  0.1143],\n",
      "         [ 0.5773, -0.8480,  1.9527],\n",
      "         [-1.2520,  1.3017,  1.2613],\n",
      "         [-1.0445,  1.8441, -0.0325],\n",
      "         [ 0.1798, -0.2022, -2.1779],\n",
      "         [ 0.8415,  0.6424,  0.2772]],\n",
      "\n",
      "        [[-1.4583,  1.5947,  0.3225],\n",
      "         [ 0.0705, -0.3765,  0.8216],\n",
      "         [-0.5176,  1.6101,  1.9239],\n",
      "         [ 0.6472,  0.3272,  1.3222],\n",
      "         [ 0.2636,  1.0815,  1.5993],\n",
      "         [-1.1916,  0.0665, -0.2726]],\n",
      "\n",
      "        [[-1.2224, -0.5033, -0.6743],\n",
      "         [ 0.1567, -0.9533,  0.4476],\n",
      "         [-0.6395, -1.8632, -1.0285],\n",
      "         [-0.4598, -1.3224,  0.7423],\n",
      "         [ 0.1798, -0.2022, -2.1779],\n",
      "         [ 2.3015, -1.2651,  0.5135]],\n",
      "\n",
      "        [[-1.8098,  0.7424,  0.6207],\n",
      "         [-0.6699,  0.5513, -0.7802],\n",
      "         [ 2.1077,  0.4353, -0.6296],\n",
      "         [ 0.2539, -0.2458, -1.6580],\n",
      "         [-0.3685, -0.9273,  0.7757],\n",
      "         [-0.3273, -0.0182, -0.7305]],\n",
      "\n",
      "        [[-0.6289, -2.4330,  1.3897],\n",
      "         [-0.1171, -1.0449, -0.3222],\n",
      "         [ 1.4995, -0.4632,  2.4232],\n",
      "         [-0.9017,  1.0286, -0.3276],\n",
      "         [-2.2010,  0.5631,  1.5038],\n",
      "         [-1.4176,  0.5850,  0.6123]],\n",
      "\n",
      "        [[-0.3088, -0.0819, -0.3525],\n",
      "         [ 0.3065, -2.8736, -0.9805],\n",
      "         [ 0.2033,  1.2263, -1.3314],\n",
      "         [-0.5528, -0.6590,  0.1132],\n",
      "         [ 1.0721,  1.1322, -0.0701],\n",
      "         [-0.7332, -0.6942, -0.2787]],\n",
      "\n",
      "        [[-0.0627,  2.0681,  0.0756],\n",
      "         [-0.6199,  0.6284, -0.9386],\n",
      "         [ 0.2452,  1.3904,  1.1741],\n",
      "         [ 1.5493, -0.2219,  0.2351],\n",
      "         [-0.9202,  0.9374,  1.2992],\n",
      "         [-0.8938,  0.6466, -1.9132]],\n",
      "\n",
      "        [[-0.3541, -0.6610,  0.8141],\n",
      "         [ 1.1842,  0.3595,  0.4083],\n",
      "         [-0.2031, -2.1825,  0.9937],\n",
      "         [ 1.8590, -1.9379, -0.5622],\n",
      "         [ 1.6621,  1.1285,  0.2236],\n",
      "         [ 0.3781,  1.4055, -0.5900]],\n",
      "\n",
      "        [[-0.3479,  1.6735,  0.0666],\n",
      "         [ 0.8318,  0.9227,  0.5877],\n",
      "         [-0.9025, -0.9395,  0.9300],\n",
      "         [-0.0471,  1.3918,  0.3314],\n",
      "         [ 0.1324, -1.1819,  0.8214],\n",
      "         [ 0.9044, -0.9969, -1.1560]],\n",
      "\n",
      "        [[ 1.3096,  0.4389, -0.0332],\n",
      "         [-1.4052,  1.1584, -1.0110],\n",
      "         [-1.4440,  0.2071,  1.0622],\n",
      "         [ 0.4594, -0.7258, -0.9091],\n",
      "         [-0.4887,  0.1315,  1.9203],\n",
      "         [ 1.6435,  0.1824, -0.6873]],\n",
      "\n",
      "        [[ 0.6403,  0.5597, -0.9339],\n",
      "         [-0.8545,  0.1500,  2.3057],\n",
      "         [ 0.0833, -0.2855, -0.6845],\n",
      "         [-0.3986, -0.3574, -0.7954],\n",
      "         [-1.1560,  0.0127, -0.1447],\n",
      "         [-1.3511, -1.0149,  0.3797]],\n",
      "\n",
      "        [[-0.6498, -0.9177,  1.0552],\n",
      "         [-2.7889, -0.0893,  0.0624],\n",
      "         [ 0.1287,  0.2699,  0.0752],\n",
      "         [-0.3480, -0.2765, -0.6060],\n",
      "         [ 0.1653, -0.0444,  0.0231],\n",
      "         [ 0.5077, -0.7475, -0.4960]],\n",
      "\n",
      "        [[ 1.3797,  1.2692,  0.3305],\n",
      "         [ 1.0366, -0.5688,  0.8215],\n",
      "         [ 1.0846,  0.7915, -1.2331],\n",
      "         [ 0.5185, -1.4740,  0.0133],\n",
      "         [-1.3552, -0.7386,  0.2372],\n",
      "         [-0.4704, -0.4787,  0.1780]],\n",
      "\n",
      "        [[ 2.2326,  0.5077, -1.2791],\n",
      "         [ 0.1567, -0.9533,  0.4476],\n",
      "         [-2.0239,  0.0718,  0.2627],\n",
      "         [-0.2400,  0.5971,  2.5062],\n",
      "         [-1.1999,  0.8317,  0.6483],\n",
      "         [ 0.1719, -0.9128, -2.1240]],\n",
      "\n",
      "        [[-0.4224, -0.5315,  0.6637],\n",
      "         [ 0.2929,  0.6262, -0.2865],\n",
      "         [ 0.3513, -0.3164,  0.6802],\n",
      "         [ 0.3489, -0.9058, -0.9043],\n",
      "         [ 0.6054,  0.3638, -1.4310],\n",
      "         [-0.5123, -1.1452, -0.1312]],\n",
      "\n",
      "        [[ 1.1924, -0.1654, -0.9297],\n",
      "         [-0.4464, -0.3906,  0.0076],\n",
      "         [-0.0684, -0.0099,  0.3219],\n",
      "         [-0.2400,  0.5971,  2.5062],\n",
      "         [-1.1146,  0.3688, -0.5181],\n",
      "         [-1.0411, -0.3860,  0.6271]],\n",
      "\n",
      "        [[ 1.0846,  0.7915, -1.2331],\n",
      "         [ 0.1236, -1.9259, -0.6820],\n",
      "         [ 1.5551, -0.3714, -0.1088],\n",
      "         [ 1.8590, -1.9379, -0.5622],\n",
      "         [ 1.5784,  0.6695, -1.5526],\n",
      "         [ 0.1288, -1.6777, -0.7035]],\n",
      "\n",
      "        [[ 0.4063,  0.5839, -0.1882],\n",
      "         [ 0.8775,  1.7942,  0.0902],\n",
      "         [ 1.0846,  0.7915, -1.2331],\n",
      "         [-1.3931, -0.1074,  1.1795],\n",
      "         [ 0.5425,  0.7088,  0.1181],\n",
      "         [ 1.1026,  1.0901, -0.2712]],\n",
      "\n",
      "        [[ 0.2720, -0.1629, -0.3434],\n",
      "         [ 0.0905,  0.7969, -0.9499],\n",
      "         [ 0.7534, -0.6100, -0.9196],\n",
      "         [ 0.5590,  2.0386,  0.1814],\n",
      "         [ 1.3249, -0.4346, -0.3119],\n",
      "         [ 0.2622,  0.3622,  0.1407]],\n",
      "\n",
      "        [[ 1.4574,  0.1274, -0.8951],\n",
      "         [ 0.5924, -1.3490, -0.4383],\n",
      "         [ 1.1543,  1.0930,  1.5823],\n",
      "         [-1.9428,  0.4447,  0.3321],\n",
      "         [-2.4725, -0.6053, -1.9144],\n",
      "         [-1.2037, -0.8774,  0.5187]],\n",
      "\n",
      "        [[-0.5750,  0.5922, -2.1918],\n",
      "         [-1.3665,  0.4570,  1.9628],\n",
      "         [ 1.0257,  0.6849, -1.0993],\n",
      "         [-0.5577, -0.8521,  0.8853],\n",
      "         [ 0.0921,  0.5906, -0.5036],\n",
      "         [-0.6931, -0.6641,  0.0326]],\n",
      "\n",
      "        [[ 0.4282, -1.4079,  0.5996],\n",
      "         [ 2.7520,  1.0935,  0.8599],\n",
      "         [ 0.1567, -0.9533,  0.4476],\n",
      "         [ 1.4604,  0.4765,  0.0313],\n",
      "         [-0.1858,  1.0062, -0.5168],\n",
      "         [ 2.1319,  0.1976,  0.3790]],\n",
      "\n",
      "        [[ 1.7153, -0.6645,  0.2334],\n",
      "         [ 1.2978,  0.2164, -0.0308],\n",
      "         [-1.2945,  1.0807, -0.0424],\n",
      "         [-0.3359,  0.2643,  1.7998],\n",
      "         [ 0.4570,  0.5547, -0.6364],\n",
      "         [ 0.1567, -0.9533,  0.4476]],\n",
      "\n",
      "        [[-1.0237,  0.1374, -1.5891],\n",
      "         [-1.2945,  1.0807, -0.0424],\n",
      "         [ 0.4932,  1.4041, -0.4472],\n",
      "         [-1.8508,  0.9928, -1.0390],\n",
      "         [ 0.8691,  2.0075, -0.8469],\n",
      "         [-0.2076, -0.5983, -0.3862]],\n",
      "\n",
      "        [[ 0.1907,  0.5416,  0.0881],\n",
      "         [-0.4584, -1.1894,  1.2500],\n",
      "         [-0.6957, -1.1788, -0.2477],\n",
      "         [ 0.3654, -0.7279,  0.3493],\n",
      "         [-1.4106, -1.7893, -1.6941],\n",
      "         [ 0.4594, -0.7258, -0.9091]],\n",
      "\n",
      "        [[-1.4005, -0.0030, -0.0370],\n",
      "         [ 0.4594, -0.7258, -0.9091],\n",
      "         [ 1.3212, -0.4998, -0.9256],\n",
      "         [ 1.0743,  1.1692,  0.3720],\n",
      "         [ 0.6215,  1.8278, -1.2155],\n",
      "         [ 0.1652,  0.2594,  0.6232]],\n",
      "\n",
      "        [[-0.4455,  0.2303,  0.5229],\n",
      "         [ 0.6843, -0.6779,  1.8189],\n",
      "         [ 0.0050,  1.5578,  2.4983],\n",
      "         [ 0.6297, -0.9133,  1.0554],\n",
      "         [ 0.2233,  0.4035,  0.6184],\n",
      "         [ 0.0652,  0.5642,  0.9893]],\n",
      "\n",
      "        [[-0.3134, -1.2866,  0.5803],\n",
      "         [-1.3044, -0.3425,  0.7455],\n",
      "         [-1.0977, -0.5798, -2.1871],\n",
      "         [-0.8077,  0.6104, -0.9346],\n",
      "         [ 0.1567, -0.9533,  0.4476],\n",
      "         [ 1.2737,  0.5519, -0.4686]],\n",
      "\n",
      "        [[ 0.8175,  1.9703, -1.3880],\n",
      "         [ 0.5590,  2.0386,  0.1814],\n",
      "         [-1.7723,  0.1507,  0.3912],\n",
      "         [ 0.7594, -0.6197, -0.4872],\n",
      "         [ 0.5089, -1.4538, -0.8659],\n",
      "         [ 0.0133, -0.0381, -0.2847]],\n",
      "\n",
      "        [[ 0.4470,  0.1932, -1.0057],\n",
      "         [-0.2454,  0.8016,  0.1442],\n",
      "         [ 0.4913, -0.6978, -0.6354],\n",
      "         [-0.2542, -0.5668,  0.4163],\n",
      "         [-0.3173,  2.0206, -0.1543],\n",
      "         [-0.8039, -1.1314,  1.4325]],\n",
      "\n",
      "        [[-1.8961, -2.2474, -0.7078],\n",
      "         [-0.3126,  0.6865,  0.8354],\n",
      "         [ 1.6825, -0.9458,  1.3864],\n",
      "         [ 2.2326,  0.5077, -1.2791],\n",
      "         [ 0.0333,  1.5179, -1.3468],\n",
      "         [-0.4816,  0.8505,  0.4928]],\n",
      "\n",
      "        [[ 0.0579,  0.3792, -1.3845],\n",
      "         [ 0.1413,  1.0135, -0.6469],\n",
      "         [ 2.4951,  1.9991,  1.8271],\n",
      "         [ 0.4416,  1.1435,  2.3003],\n",
      "         [-0.8838,  0.1247,  0.8685],\n",
      "         [ 0.3496,  0.3674,  0.9674]]], grad_fn=<EmbeddingBackward>)\n",
      "torch.Size([32, 6, 3])\n",
      "ret= tensor([[[-2.5304e-02, -7.9664e-01, -5.2643e-01,  4.0416e-02,  8.8332e-01,\n",
      "          -2.0883e-02]],\n",
      "\n",
      "        [[-1.2308e-01,  2.0079e-01, -3.0729e-01, -2.3556e-01, -3.8659e-01,\n",
      "           3.5296e-01]],\n",
      "\n",
      "        [[ 6.3135e-01,  3.4145e-01,  1.1993e+00,  6.0975e-01,  4.1341e-01,\n",
      "          -1.6344e-02]],\n",
      "\n",
      "        [[-8.1369e-02,  3.4322e-01,  7.5325e-01,  2.9315e-01, -7.2747e-01,\n",
      "           9.6444e-02]],\n",
      "\n",
      "        [[ 5.0787e-02,  2.3918e-01,  6.5070e-02, -4.0459e-01, -1.0278e+00,\n",
      "          -6.3976e-01]],\n",
      "\n",
      "        [[ 2.4892e-01, -8.4697e-01,  2.8764e-01,  2.4205e-01, -5.2103e-01,\n",
      "           4.0969e-01]],\n",
      "\n",
      "        [[-1.2742e-03,  9.4689e-02,  5.5205e-02, -5.8469e-01,  5.5268e-01,\n",
      "           5.5597e-02]],\n",
      "\n",
      "        [[ 1.4568e-01, -3.2958e-01,  7.1540e-01,  7.2584e-01, -6.4950e-01,\n",
      "          -4.9235e-01]],\n",
      "\n",
      "        [[ 1.1654e-01, -1.2718e+00,  5.5127e-01, -2.9557e-01, -4.2127e-01,\n",
      "          -1.9806e-01]],\n",
      "\n",
      "        [[ 7.0063e-01, -9.3905e-01, -4.2819e-01, -7.4857e-02,  3.0945e-01,\n",
      "           6.6720e-01]],\n",
      "\n",
      "        [[ 1.6161e-01, -3.7836e-01,  1.1448e-01,  1.7909e-01,  1.4126e-01,\n",
      "          -7.3937e-03]],\n",
      "\n",
      "        [[ 1.1094e-01, -2.0914e+00, -1.6750e-02, -2.3668e-01,  1.5201e-01,\n",
      "           6.6306e-01]],\n",
      "\n",
      "        [[-1.6163e+00, -7.1156e-01, -2.7247e-01,  5.3753e-01,  9.7384e-01,\n",
      "           4.0436e-01]],\n",
      "\n",
      "        [[-5.7504e-01,  5.1017e-01,  2.1100e-01, -2.3773e-01, -2.9278e-01,\n",
      "           4.2658e-01]],\n",
      "\n",
      "        [[ 1.3895e-01, -1.0901e-01,  7.6975e-02,  3.6719e-04, -1.9841e-01,\n",
      "           1.3981e-01]],\n",
      "\n",
      "        [[-5.5223e-01,  1.0813e-01,  1.1829e-01,  8.4191e-01,  6.6175e-02,\n",
      "           4.3536e-01]],\n",
      "\n",
      "        [[ 1.2514e+00, -1.0196e+00,  6.5013e-01, -7.9897e-02,  1.4934e+00,\n",
      "          -8.6194e-01]],\n",
      "\n",
      "        [[ 3.1540e-01,  8.0665e-01,  6.6581e-01, -3.0021e-01,  2.8582e-01,\n",
      "           5.4882e-01]],\n",
      "\n",
      "        [[-1.7387e-01, -5.1094e-01, -4.4911e-01, -2.7253e-01, -3.1999e-01,\n",
      "          -3.2671e-02]],\n",
      "\n",
      "        [[ 6.0645e-01, -7.0756e-01,  1.7136e+00, -6.1248e-01, -2.1438e+00,\n",
      "          -1.0715e+00]],\n",
      "\n",
      "        [[ 1.3059e-01,  1.1828e+00, -5.7221e-01,  2.3537e-01,  1.8541e-02,\n",
      "           2.2616e-01]],\n",
      "\n",
      "        [[ 1.1375e-01, -3.6046e-01,  8.4996e-02, -1.6270e-01, -8.6540e-02,\n",
      "          -1.9103e-01]],\n",
      "\n",
      "        [[-4.3856e-01, -2.0221e-01,  6.8295e-01,  1.6609e+00, -3.8626e-01,\n",
      "          -6.2007e-02]],\n",
      "\n",
      "        [[ 1.3831e+00,  3.0555e-01, -3.9776e-01,  1.1858e+00, -5.2310e-01,\n",
      "           5.1008e-01]],\n",
      "\n",
      "        [[ 1.4077e-01, -7.2940e-01, -1.9261e-01, -5.4456e-01,  2.2645e-01,\n",
      "          -1.9776e-01]],\n",
      "\n",
      "        [[-5.1144e-01,  4.9243e-02,  3.0112e-01,  2.3347e-01, -5.6572e-01,\n",
      "           1.9189e-01]],\n",
      "\n",
      "        [[ 1.8492e-01, -4.1614e-01,  5.5861e-01, -4.8620e-01,  9.3545e-02,\n",
      "           1.8724e-01]],\n",
      "\n",
      "        [[-5.6186e-01,  1.3268e-01,  3.9910e-01,  6.5385e-01, -5.3670e-01,\n",
      "          -7.1495e-02]],\n",
      "\n",
      "        [[-7.4189e-01, -3.9557e-01,  2.5983e-02,  3.6352e-02,  1.2677e-01,\n",
      "          -5.7781e-02]],\n",
      "\n",
      "        [[ 3.5029e-01,  2.8917e-01, -1.2711e-01, -2.6789e-01,  8.4698e-01,\n",
      "          -6.9520e-01]],\n",
      "\n",
      "        [[ 1.0011e+00, -7.5886e-01,  1.0433e-01,  7.7670e-01, -6.4011e-02,\n",
      "          -7.0676e-01]],\n",
      "\n",
      "        [[ 2.2711e-01, -1.6275e-01, -5.3522e-01, -8.7227e-01, -4.8754e-01,\n",
      "          -2.8466e-01]]], grad_fn=<UnsafeViewBackward>)\n",
      "tensor([[1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0]])\n",
      "torch.Size([32, 1, 6]) torch.Size([32, 6])\n",
      "ret.view= tensor([[-2.5304e-02, -7.9664e-01, -5.2643e-01,  4.0416e-02,  8.8332e-01,\n",
      "         -2.0883e-02],\n",
      "        [-1.2308e-01,  2.0079e-01, -3.0729e-01, -2.3556e-01, -3.8659e-01,\n",
      "          3.5296e-01],\n",
      "        [ 6.3135e-01,  3.4145e-01,  1.1993e+00,  6.0975e-01,  4.1341e-01,\n",
      "         -1.6344e-02],\n",
      "        [-8.1369e-02,  3.4322e-01,  7.5325e-01,  2.9315e-01, -7.2747e-01,\n",
      "          9.6444e-02],\n",
      "        [ 5.0787e-02,  2.3918e-01,  6.5070e-02, -4.0459e-01, -1.0278e+00,\n",
      "         -6.3976e-01],\n",
      "        [ 2.4892e-01, -8.4697e-01,  2.8764e-01,  2.4205e-01, -5.2103e-01,\n",
      "          4.0969e-01],\n",
      "        [-1.2742e-03,  9.4689e-02,  5.5205e-02, -5.8469e-01,  5.5268e-01,\n",
      "          5.5597e-02],\n",
      "        [ 1.4568e-01, -3.2958e-01,  7.1540e-01,  7.2584e-01, -6.4950e-01,\n",
      "         -4.9235e-01],\n",
      "        [ 1.1654e-01, -1.2718e+00,  5.5127e-01, -2.9557e-01, -4.2127e-01,\n",
      "         -1.9806e-01],\n",
      "        [ 7.0063e-01, -9.3905e-01, -4.2819e-01, -7.4857e-02,  3.0945e-01,\n",
      "          6.6720e-01],\n",
      "        [ 1.6161e-01, -3.7836e-01,  1.1448e-01,  1.7909e-01,  1.4126e-01,\n",
      "         -7.3937e-03],\n",
      "        [ 1.1094e-01, -2.0914e+00, -1.6750e-02, -2.3668e-01,  1.5201e-01,\n",
      "          6.6306e-01],\n",
      "        [-1.6163e+00, -7.1156e-01, -2.7247e-01,  5.3753e-01,  9.7384e-01,\n",
      "          4.0436e-01],\n",
      "        [-5.7504e-01,  5.1017e-01,  2.1100e-01, -2.3773e-01, -2.9278e-01,\n",
      "          4.2658e-01],\n",
      "        [ 1.3895e-01, -1.0901e-01,  7.6975e-02,  3.6719e-04, -1.9841e-01,\n",
      "          1.3981e-01],\n",
      "        [-5.5223e-01,  1.0813e-01,  1.1829e-01,  8.4191e-01,  6.6175e-02,\n",
      "          4.3536e-01],\n",
      "        [ 1.2514e+00, -1.0196e+00,  6.5013e-01, -7.9897e-02,  1.4934e+00,\n",
      "         -8.6194e-01],\n",
      "        [ 3.1540e-01,  8.0665e-01,  6.6581e-01, -3.0021e-01,  2.8582e-01,\n",
      "          5.4882e-01],\n",
      "        [-1.7387e-01, -5.1094e-01, -4.4911e-01, -2.7253e-01, -3.1999e-01,\n",
      "         -3.2671e-02],\n",
      "        [ 6.0645e-01, -7.0756e-01,  1.7136e+00, -6.1248e-01, -2.1438e+00,\n",
      "         -1.0715e+00],\n",
      "        [ 1.3059e-01,  1.1828e+00, -5.7221e-01,  2.3537e-01,  1.8541e-02,\n",
      "          2.2616e-01],\n",
      "        [ 1.1375e-01, -3.6046e-01,  8.4996e-02, -1.6270e-01, -8.6540e-02,\n",
      "         -1.9103e-01],\n",
      "        [-4.3856e-01, -2.0221e-01,  6.8295e-01,  1.6609e+00, -3.8626e-01,\n",
      "         -6.2007e-02],\n",
      "        [ 1.3831e+00,  3.0555e-01, -3.9776e-01,  1.1858e+00, -5.2310e-01,\n",
      "          5.1008e-01],\n",
      "        [ 1.4077e-01, -7.2940e-01, -1.9261e-01, -5.4456e-01,  2.2645e-01,\n",
      "         -1.9776e-01],\n",
      "        [-5.1144e-01,  4.9243e-02,  3.0112e-01,  2.3347e-01, -5.6572e-01,\n",
      "          1.9189e-01],\n",
      "        [ 1.8492e-01, -4.1614e-01,  5.5861e-01, -4.8620e-01,  9.3545e-02,\n",
      "          1.8724e-01],\n",
      "        [-5.6186e-01,  1.3268e-01,  3.9910e-01,  6.5385e-01, -5.3670e-01,\n",
      "         -7.1495e-02],\n",
      "        [-7.4189e-01, -3.9557e-01,  2.5983e-02,  3.6352e-02,  1.2677e-01,\n",
      "         -5.7781e-02],\n",
      "        [ 3.5029e-01,  2.8917e-01, -1.2711e-01, -2.6789e-01,  8.4698e-01,\n",
      "         -6.9520e-01],\n",
      "        [ 1.0011e+00, -7.5886e-01,  1.0433e-01,  7.7670e-01, -6.4011e-02,\n",
      "         -7.0676e-01],\n",
      "        [ 2.2711e-01, -1.6275e-01, -5.3522e-01, -8.7227e-01, -4.8754e-01,\n",
      "         -2.8466e-01]], grad_fn=<ViewBackward>)\n",
      "label= tensor([[1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0]])\n",
      "h_list tensor([[[ 0.0679,  0.0533, -0.4049]],\n",
      "\n",
      "        [[-0.3389, -0.4049,  0.0879]],\n",
      "\n",
      "        [[-0.2306, -0.4735, -0.1649]],\n",
      "\n",
      "        [[ 0.1817,  0.5228, -0.2265]],\n",
      "\n",
      "        [[ 0.2860, -0.2027, -0.1889]],\n",
      "\n",
      "        [[-0.7556,  0.2490, -0.1021]],\n",
      "\n",
      "        [[-0.4033, -0.0184,  0.1531]],\n",
      "\n",
      "        [[-0.0898, -0.4057, -0.1895]],\n",
      "\n",
      "        [[-1.0233, -0.1223, -0.5238]],\n",
      "\n",
      "        [[ 0.5187,  0.0703,  0.2884]],\n",
      "\n",
      "        [[-0.0961,  0.0590, -0.2035]],\n",
      "\n",
      "        [[ 0.7688, -0.4748,  0.1657]],\n",
      "\n",
      "        [[-0.5207, -0.5532, -0.5921]],\n",
      "\n",
      "        [[-0.1205, -0.5441,  0.0232]],\n",
      "\n",
      "        [[-0.0516, -0.1092,  0.0890]],\n",
      "\n",
      "        [[-0.2162, -0.0234,  0.3208]],\n",
      "\n",
      "        [[ 0.5550,  0.6124, -0.1336]],\n",
      "\n",
      "        [[-0.0529,  0.4892, -0.2725]],\n",
      "\n",
      "        [[-0.1825, -0.1209,  0.4191]],\n",
      "\n",
      "        [[ 0.5107,  0.6679,  0.2491]],\n",
      "\n",
      "        [[-0.5593,  0.2507,  0.1549]],\n",
      "\n",
      "        [[-0.0718, -0.1189, -0.0382]],\n",
      "\n",
      "        [[-0.2059,  0.4176,  0.8231]],\n",
      "\n",
      "        [[-0.4722, -0.3062, -0.5927]],\n",
      "\n",
      "        [[-0.3454,  0.4304, -0.3007]],\n",
      "\n",
      "        [[ 0.3576, -0.2266,  0.3074]],\n",
      "\n",
      "        [[-0.2311,  0.3673, -0.0050]],\n",
      "\n",
      "        [[-0.3028,  0.4437, -0.1481]],\n",
      "\n",
      "        [[ 0.0181, -0.2198,  0.2332]],\n",
      "\n",
      "        [[-0.4347,  0.3142, -0.4811]],\n",
      "\n",
      "        [[ 0.1866, -0.4571, -0.4630]],\n",
      "\n",
      "        [[ 0.2548, -0.3563, -0.2510]]], grad_fn=<CatBackward>)\n",
      "out_emb tensor([[[ 0.1500,  0.2027,  0.1143]],\n",
      "\n",
      "        [[-1.4583,  1.5947,  0.3225]],\n",
      "\n",
      "        [[-1.2224, -0.5033, -0.6743]],\n",
      "\n",
      "        [[-1.8098,  0.7424,  0.6207]],\n",
      "\n",
      "        [[-0.6289, -2.4330,  1.3897]],\n",
      "\n",
      "        [[-0.3088, -0.0819, -0.3525]],\n",
      "\n",
      "        [[-0.0627,  2.0681,  0.0756]],\n",
      "\n",
      "        [[-0.3541, -0.6610,  0.8141]],\n",
      "\n",
      "        [[-0.3479,  1.6735,  0.0666]],\n",
      "\n",
      "        [[ 1.3096,  0.4389, -0.0332]],\n",
      "\n",
      "        [[ 0.6403,  0.5597, -0.9339]],\n",
      "\n",
      "        [[-0.6498, -0.9177,  1.0552]],\n",
      "\n",
      "        [[ 1.3797,  1.2692,  0.3305]],\n",
      "\n",
      "        [[ 2.2326,  0.5077, -1.2791]],\n",
      "\n",
      "        [[-0.4224, -0.5315,  0.6637]],\n",
      "\n",
      "        [[ 1.1924, -0.1654, -0.9297]],\n",
      "\n",
      "        [[ 1.0846,  0.7915, -1.2331]],\n",
      "\n",
      "        [[ 0.4063,  0.5839, -0.1882]],\n",
      "\n",
      "        [[ 0.2720, -0.1629, -0.3434]],\n",
      "\n",
      "        [[ 1.4574,  0.1274, -0.8951]],\n",
      "\n",
      "        [[-0.5750,  0.5922, -2.1918]],\n",
      "\n",
      "        [[ 0.4282, -1.4079,  0.5996]],\n",
      "\n",
      "        [[ 1.7153, -0.6645,  0.2334]],\n",
      "\n",
      "        [[-1.0237,  0.1374, -1.5891]],\n",
      "\n",
      "        [[ 0.1907,  0.5416,  0.0881]],\n",
      "\n",
      "        [[-1.4005, -0.0030, -0.0370]],\n",
      "\n",
      "        [[-0.4455,  0.2303,  0.5229]],\n",
      "\n",
      "        [[-0.3134, -1.2866,  0.5803]],\n",
      "\n",
      "        [[ 0.8175,  1.9703, -1.3880]],\n",
      "\n",
      "        [[ 0.4470,  0.1932, -1.0057]],\n",
      "\n",
      "        [[-1.8961, -2.2474, -0.7078]],\n",
      "\n",
      "        [[ 0.0579,  0.3792, -1.3845]]], grad_fn=<EmbeddingBackward>)\n",
      "h * out mul tensor([[[ 1.0184e-02,  1.0792e-02, -4.6280e-02]],\n",
      "\n",
      "        [[ 4.9424e-01, -6.4569e-01,  2.8361e-02]],\n",
      "\n",
      "        [[ 2.8186e-01,  2.3830e-01,  1.1119e-01]],\n",
      "\n",
      "        [[-3.2892e-01,  3.8813e-01, -1.4059e-01]],\n",
      "\n",
      "        [[-1.7991e-01,  4.9324e-01, -2.6254e-01]],\n",
      "\n",
      "        [[ 2.3334e-01, -2.0401e-02,  3.5987e-02]],\n",
      "\n",
      "        [[ 2.5304e-02, -3.8154e-02,  1.1576e-02]],\n",
      "\n",
      "        [[ 3.1796e-02,  2.6817e-01, -1.5428e-01]],\n",
      "\n",
      "        [[ 3.5603e-01, -2.0459e-01, -3.4903e-02]],\n",
      "\n",
      "        [[ 6.7935e-01,  3.0856e-02, -9.5768e-03]],\n",
      "\n",
      "        [[-6.1512e-02,  3.3032e-02,  1.9009e-01]],\n",
      "\n",
      "        [[-4.9957e-01,  4.3571e-01,  1.7481e-01]],\n",
      "\n",
      "        [[-7.1848e-01, -7.0215e-01, -1.9572e-01]],\n",
      "\n",
      "        [[-2.6910e-01, -2.7623e-01, -2.9709e-02]],\n",
      "\n",
      "        [[ 2.1813e-02,  5.8038e-02,  5.9100e-02]],\n",
      "\n",
      "        [[-2.5785e-01,  3.8756e-03, -2.9825e-01]],\n",
      "\n",
      "        [[ 6.0194e-01,  4.8468e-01,  1.6474e-01]],\n",
      "\n",
      "        [[-2.1507e-02,  2.8562e-01,  5.1283e-02]],\n",
      "\n",
      "        [[-4.9645e-02,  1.9697e-02, -1.4392e-01]],\n",
      "\n",
      "        [[ 7.4431e-01,  8.5082e-02, -2.2294e-01]],\n",
      "\n",
      "        [[ 3.2158e-01,  1.4845e-01, -3.3944e-01]],\n",
      "\n",
      "        [[-3.0735e-02,  1.6741e-01, -2.2929e-02]],\n",
      "\n",
      "        [[-3.5321e-01, -2.7748e-01,  1.9213e-01]],\n",
      "\n",
      "        [[ 4.8340e-01, -4.2063e-02,  9.4180e-01]],\n",
      "\n",
      "        [[-6.5865e-02,  2.3313e-01, -2.6497e-02]],\n",
      "\n",
      "        [[-5.0074e-01,  6.8144e-04, -1.1384e-02]],\n",
      "\n",
      "        [[ 1.0294e-01,  8.4587e-02, -2.5987e-03]],\n",
      "\n",
      "        [[ 9.4898e-02, -5.7082e-01, -8.5944e-02]],\n",
      "\n",
      "        [[ 1.4829e-02, -4.3301e-01, -3.2371e-01]],\n",
      "\n",
      "        [[-1.9430e-01,  6.0702e-02,  4.8389e-01]],\n",
      "\n",
      "        [[-3.5377e-01,  1.0272e+00,  3.2768e-01]],\n",
      "\n",
      "        [[ 1.4761e-02, -1.3511e-01,  3.4746e-01]]], grad_fn=<MulBackward0>)\n",
      "h * out mul sum(0) tensor([[0.6275, 1.2117, 0.7689]], grad_fn=<SumBackward1>)\n",
      "h * out mul sum(1)：按行求和= tensor([[ 1.0184e-02,  1.0792e-02, -4.6280e-02],\n",
      "        [ 4.9424e-01, -6.4569e-01,  2.8361e-02],\n",
      "        [ 2.8186e-01,  2.3830e-01,  1.1119e-01],\n",
      "        [-3.2892e-01,  3.8813e-01, -1.4059e-01],\n",
      "        [-1.7991e-01,  4.9324e-01, -2.6254e-01],\n",
      "        [ 2.3334e-01, -2.0401e-02,  3.5987e-02],\n",
      "        [ 2.5304e-02, -3.8154e-02,  1.1576e-02],\n",
      "        [ 3.1796e-02,  2.6817e-01, -1.5428e-01],\n",
      "        [ 3.5603e-01, -2.0459e-01, -3.4903e-02],\n",
      "        [ 6.7935e-01,  3.0856e-02, -9.5768e-03],\n",
      "        [-6.1512e-02,  3.3032e-02,  1.9009e-01],\n",
      "        [-4.9957e-01,  4.3571e-01,  1.7481e-01],\n",
      "        [-7.1848e-01, -7.0215e-01, -1.9572e-01],\n",
      "        [-2.6910e-01, -2.7623e-01, -2.9709e-02],\n",
      "        [ 2.1813e-02,  5.8038e-02,  5.9100e-02],\n",
      "        [-2.5785e-01,  3.8756e-03, -2.9825e-01],\n",
      "        [ 6.0194e-01,  4.8468e-01,  1.6474e-01],\n",
      "        [-2.1507e-02,  2.8562e-01,  5.1283e-02],\n",
      "        [-4.9645e-02,  1.9697e-02, -1.4392e-01],\n",
      "        [ 7.4431e-01,  8.5082e-02, -2.2294e-01],\n",
      "        [ 3.2158e-01,  1.4845e-01, -3.3944e-01],\n",
      "        [-3.0735e-02,  1.6741e-01, -2.2929e-02],\n",
      "        [-3.5321e-01, -2.7748e-01,  1.9213e-01],\n",
      "        [ 4.8340e-01, -4.2063e-02,  9.4180e-01],\n",
      "        [-6.5865e-02,  2.3313e-01, -2.6497e-02],\n",
      "        [-5.0074e-01,  6.8144e-04, -1.1384e-02],\n",
      "        [ 1.0294e-01,  8.4587e-02, -2.5987e-03],\n",
      "        [ 9.4898e-02, -5.7082e-01, -8.5944e-02],\n",
      "        [ 1.4829e-02, -4.3301e-01, -3.2371e-01],\n",
      "        [-1.9430e-01,  6.0702e-02,  4.8389e-01],\n",
      "        [-3.5377e-01,  1.0272e+00,  3.2768e-01],\n",
      "        [ 1.4761e-02, -1.3511e-01,  3.4746e-01]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "for i, (center_batch, context_batch, center_ns_batch, context_len_batch, label_batch) in enumerate(dataloader2):\n",
    "    \n",
    "    print(\"i=%d\" % (i))\n",
    "    #print(center_batch)\n",
    "    #print(context_batch)\n",
    "    #print(ns_batch)\n",
    "    #print(context_len_batch)\n",
    "    \n",
    "    #print(ns_len_batch)\n",
    "    #print(\"pre_h\") \n",
    "    #h = pre_h2(in_embed, context_batch, context_len_batch)\n",
    "    #print(\"h=\",h)\n",
    "    \n",
    "    h = pre_h(in_embed, context_batch, context_len_batch)\n",
    "    #print(\"h=\",h)\n",
    "    #if i == 0:\n",
    "    #    break\n",
    "    \n",
    "    # test: \n",
    "    out_neg_w = out_embed(center_ns_batch)\n",
    "    \n",
    "    print(out_neg_w)\n",
    "    \n",
    "    print(out_neg_w.shape)\n",
    "    \n",
    "    ret = torch.matmul(h, out_neg_w.permute(0, 2, 1))\n",
    "    print(\"ret=\", ret)\n",
    "    \n",
    "    \n",
    "    print(label_batch)\n",
    "    print(ret.shape, label_batch.shape)\n",
    "    print(\"ret.view=\", ret.view(label_batch.shape))\n",
    "    \n",
    "    print(\"label=\", label_batch)\n",
    "    \n",
    "    \n",
    "    out_cent = out_embed(center_batch)\n",
    "    \n",
    "    print(\"h_list\",h)\n",
    "    \n",
    "    print(\"out_emb\",out_cent)\n",
    "    \n",
    "    #print(\"out_emb trans\",out_cent.permute(1, 0))\n",
    "    \n",
    "    # 2维可以\n",
    "    #print(out_cent.t())\n",
    "\n",
    "    print(\"h * out mul\",torch.mul(h, out_cent))\n",
    "    \n",
    "    print(\"h * out mul sum(0)\",torch.mul(h, out_cent).sum(0))\n",
    "    # 按行求和\n",
    "    print(\"h * out mul sum(1)：按行求和=\",torch.mul(h, out_cent).sum(1))\n",
    "    \n",
    "    #print(\"h * out matmul\",torch.matmul(h, out_cent.permute(1, 0)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]],\n",
       "\n",
       "        [[1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.tensor([[1, 0, 0, 0, 0, 0],\n",
    "        [1, 0, 0, 0, 0, 0]])\n",
    "k.unsqueeze(2\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2642178"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = np.array([ 0.5038, -0.1116, -0.1281])\n",
    "b = np.array([-0.3235, -0.1992,  0.5401])\n",
    "\n",
    "c = np.array([-0.6835,  1.9188, -0.1909])\n",
    "np.dot(b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(4, 4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6938, 0.4322, 0.1372, 0.5597],\n",
       "         [0.4322, 1.0403, 0.2153, 0.4649],\n",
       "         [0.1372, 0.2153, 0.1716, 0.0940],\n",
       "         [0.5597, 0.4649, 0.0940, 0.4797]],\n",
       "\n",
       "        [[0.9739, 0.8339, 1.2277, 0.5855],\n",
       "         [0.8339, 1.0014, 1.2593, 0.7542],\n",
       "         [1.2277, 1.2593, 1.7990, 0.8382],\n",
       "         [0.5855, 0.7542, 0.8382, 0.6427]],\n",
       "\n",
       "        [[0.5103, 0.4979, 0.7008, 0.8078],\n",
       "         [0.4979, 0.5169, 0.6957, 0.7483],\n",
       "         [0.7008, 0.6957, 1.0871, 1.1403],\n",
       "         [0.8078, 0.7483, 1.1403, 1.3479]],\n",
       "\n",
       "        [[1.4820, 1.1996, 0.6819, 1.0098],\n",
       "         [1.1996, 1.7690, 0.6960, 1.0845],\n",
       "         [0.6819, 0.6960, 0.8283, 0.2715],\n",
       "         [1.0098, 1.0845, 0.2715, 0.8967]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(tensor,tensor.permute(0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[0.5365, 0.4129, 0.6282],\n",
    "          [0.7699, 0.1266, 0.2930],\n",
    "          [0.8473, 0.5407, 0.7543],\n",
    "          [0.8037, 0.3583, 0.7042]])\n",
    "b = torch.tensor([[0.5365, 0.7699, 0.8473, 0.8037],\n",
    "          [0.4129, 0.1266, 0.5407, 0.3583],\n",
    "          [0.6282, 0.2930, 0.7543, 0.7042]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8530, 0.6494, 1.1517, 1.0215],\n",
       "        [0.6494, 0.6946, 0.9418, 0.8705],\n",
       "        [1.1517, 0.9418, 1.5792, 1.4059],\n",
       "        [1.0215, 0.8705, 1.4059, 1.2702]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8529538999999999"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5365 * 0.5365 + 0.4129 * 0.4129 + 0.6282 * 0.6282"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2VModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(W2VModel, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.in_embed = torch.nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        self.out_embed = torch.nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        \n",
    "    def forward(self, centers, contexts, center_negatives, real_context_len, labels):\n",
    "        ''' \n",
    "            forward最后一层只需要计算抽样的正负例\n",
    "            其中中心词对应label=1,neg对应负例\n",
    "        '''\n",
    "        \n",
    "        # 调整batch数据\n",
    "        h_list = None\n",
    "        # 处理batch数据:因为长度不一致，无法使用sum(0)去按列求和函数\n",
    "        for i in range(len(centers)):\n",
    "            h = self.in_embed(contexts[i][0: real_context_len[i]]).sum(0) / real_context_len[i]\n",
    "            h = torch.unsqueeze(h, 0)\n",
    "            h = torch.unsqueeze(h, 0)\n",
    "            #print(\"i=\", i)\n",
    "            #print(\"h=\", h)\n",
    "            if h_list is None:\n",
    "                h_list = h\n",
    "            else:\n",
    "                h_list = torch.cat((h_list, h), 0)\n",
    "        \n",
    "        #labels = torch.unsqueeze(labels, 0)\n",
    "        #print(\"h_list=\", h_list)\n",
    "        out_neg_w = self.out_embed(center_negatives)\n",
    "\n",
    "        ret = torch.matmul(h_list, out_neg_w.permute(0, 2, 1))\n",
    "        #print(\"label size=\", label_batch.shape)\n",
    "        \n",
    "        #print(\"ret size=\", ret.shape)\n",
    "        ret = ret.squeeze()\n",
    "        #print(\"ret size=\", ret.shape)\n",
    "        #print(\"ret\", ret)\n",
    "        #print(\"label\", labels)\n",
    "        ret = ret.view(label_batch.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(ret.float(), labels.float(), reduction=\"none\")\n",
    "       \n",
    "        return loss.sum(dim=1).mean()\n",
    "net = W2VModel(len(idx2word), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def train(net, lr, num_epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"train on\", device)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        start, l_sum, n = time.time(), 0.0, 0\n",
    "        for i, batch in enumerate(dataloader2):\n",
    "            center_batch, context_batch, center_ns_batch, context_len_batch, label_batch = [d.to(device) for d in batch]\n",
    "            \n",
    "            loss = net(center_batch, context_batch, center_ns_batch, context_len_batch, label_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            l_sum += loss.cpu().item()\n",
    "            n += 1\n",
    "        print('epoch %d, loss %.2f, time %.2fs'\n",
    "              % (epoch + 1, l_sum / n, time.time() - start)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on cuda\n",
      "epoch 1, loss 4.04, time 10.92s\n",
      "epoch 2, loss 2.79, time 10.78s\n",
      "epoch 3, loss 2.61, time 10.74s\n",
      "epoch 4, loss 2.55, time 10.65s\n",
      "epoch 5, loss 2.49, time 10.67s\n",
      "epoch 6, loss 2.44, time 10.80s\n",
      "epoch 7, loss 2.40, time 10.67s\n",
      "epoch 8, loss 2.37, time 10.66s\n",
      "epoch 9, loss 2.34, time 10.58s\n",
      "epoch 10, loss 2.31, time 10.41s\n",
      "epoch 11, loss 2.29, time 10.53s\n",
      "epoch 12, loss 2.27, time 10.83s\n",
      "epoch 13, loss 2.25, time 10.75s\n",
      "epoch 14, loss 2.23, time 10.75s\n",
      "epoch 15, loss 2.22, time 10.61s\n",
      "epoch 16, loss 2.21, time 10.71s\n",
      "epoch 17, loss 2.19, time 10.72s\n",
      "epoch 18, loss 2.18, time 10.67s\n",
      "epoch 19, loss 2.17, time 10.80s\n",
      "epoch 20, loss 2.16, time 10.73s\n",
      "epoch 21, loss 2.15, time 10.65s\n",
      "epoch 22, loss 2.14, time 10.79s\n",
      "epoch 23, loss 2.13, time 11.35s\n",
      "epoch 24, loss 2.12, time 11.41s\n",
      "epoch 25, loss 2.12, time 10.82s\n",
      "epoch 26, loss 2.11, time 10.61s\n",
      "epoch 27, loss 2.10, time 10.73s\n",
      "epoch 28, loss 2.10, time 10.75s\n",
      "epoch 29, loss 2.09, time 10.82s\n",
      "epoch 30, loss 2.09, time 10.80s\n",
      "epoch 31, loss 2.08, time 10.81s\n",
      "epoch 32, loss 2.08, time 10.83s\n",
      "epoch 33, loss 2.07, time 10.80s\n",
      "epoch 34, loss 2.07, time 10.71s\n",
      "epoch 35, loss 2.06, time 10.79s\n",
      "epoch 36, loss 2.06, time 10.71s\n",
      "epoch 37, loss 2.05, time 10.58s\n",
      "epoch 38, loss 2.05, time 10.43s\n",
      "epoch 39, loss 2.05, time 10.74s\n",
      "epoch 40, loss 2.04, time 10.70s\n",
      "epoch 41, loss 2.04, time 10.65s\n",
      "epoch 42, loss 2.04, time 10.74s\n",
      "epoch 43, loss 2.03, time 11.00s\n",
      "epoch 44, loss 2.03, time 11.54s\n",
      "epoch 45, loss 2.03, time 11.10s\n",
      "epoch 46, loss 2.02, time 10.70s\n",
      "epoch 47, loss 2.02, time 10.83s\n",
      "epoch 48, loss 2.02, time 10.81s\n",
      "epoch 49, loss 2.02, time 10.83s\n",
      "epoch 50, loss 2.01, time 10.80s\n",
      "epoch 51, loss 2.01, time 10.73s\n",
      "epoch 52, loss 2.01, time 10.73s\n",
      "epoch 53, loss 2.01, time 10.77s\n",
      "epoch 54, loss 2.00, time 10.67s\n",
      "epoch 55, loss 2.00, time 10.65s\n",
      "epoch 56, loss 2.00, time 10.69s\n",
      "epoch 57, loss 2.00, time 10.82s\n",
      "epoch 58, loss 2.00, time 10.75s\n",
      "epoch 59, loss 1.99, time 10.78s\n",
      "epoch 60, loss 1.99, time 10.78s\n",
      "epoch 61, loss 1.99, time 10.79s\n",
      "epoch 62, loss 1.99, time 10.67s\n",
      "epoch 63, loss 1.98, time 10.74s\n",
      "epoch 64, loss 1.98, time 10.64s\n",
      "epoch 65, loss 1.98, time 10.73s\n",
      "epoch 66, loss 1.98, time 10.81s\n",
      "epoch 67, loss 1.98, time 10.85s\n",
      "epoch 68, loss 1.98, time 10.66s\n",
      "epoch 69, loss 1.97, time 10.77s\n",
      "epoch 70, loss 1.97, time 10.72s\n",
      "epoch 71, loss 1.97, time 10.70s\n",
      "epoch 72, loss 1.97, time 10.64s\n",
      "epoch 73, loss 1.97, time 10.86s\n",
      "epoch 74, loss 1.97, time 10.80s\n",
      "epoch 75, loss 1.97, time 10.74s\n",
      "epoch 76, loss 1.96, time 10.79s\n",
      "epoch 77, loss 1.96, time 10.78s\n",
      "epoch 78, loss 1.96, time 10.78s\n",
      "epoch 79, loss 1.96, time 10.81s\n",
      "epoch 80, loss 1.96, time 10.79s\n",
      "epoch 81, loss 1.96, time 10.72s\n",
      "epoch 82, loss 1.95, time 10.64s\n",
      "epoch 83, loss 1.95, time 10.75s\n",
      "epoch 84, loss 1.95, time 10.80s\n",
      "epoch 85, loss 1.95, time 10.57s\n",
      "epoch 86, loss 1.95, time 10.62s\n",
      "epoch 87, loss 1.95, time 10.67s\n",
      "epoch 88, loss 1.95, time 10.74s\n",
      "epoch 89, loss 1.95, time 10.74s\n",
      "epoch 90, loss 1.94, time 10.75s\n",
      "epoch 91, loss 1.94, time 10.82s\n",
      "epoch 92, loss 1.94, time 10.71s\n",
      "epoch 93, loss 1.94, time 10.75s\n",
      "epoch 94, loss 1.94, time 10.75s\n",
      "epoch 95, loss 1.94, time 10.78s\n",
      "epoch 96, loss 1.94, time 10.70s\n",
      "epoch 97, loss 1.94, time 10.82s\n",
      "epoch 98, loss 1.94, time 10.78s\n",
      "epoch 99, loss 1.94, time 10.66s\n",
      "epoch 100, loss 1.93, time 10.71s\n"
     ]
    }
   ],
   "source": [
    "train(net, 0.01, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=1.000: 岂知\n",
      "cosine sim=1.000: 曹爽\n",
      "cosine sim=1.000: 孙仲谋\n",
      "cosine sim=1.000: 想\n",
      "cosine sim=1.000: 发落\n",
      "cosine sim=1.000: 除非\n",
      "cosine sim=1.000: 昏绝\n",
      "cosine sim=0.999: 性\n",
      "cosine sim=0.999: 米\n",
      "cosine sim=0.999: 相迎\n"
     ]
    }
   ],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data\n",
    "    x = W[word2idx[query_token]]\n",
    "    # 添加的1e-9是为了数值稳定性\n",
    "    cos = torch.matmul(W, x) / (torch.sum(W * W, dim=1) * torch.sum(x * x) + 1e-9).sqrt()\n",
    "    _, topk = torch.topk(cos, k=k+1)\n",
    "    topk = topk.cpu().numpy()\n",
    "    for i in topk[1:]:  # 除去输入词\n",
    "        print('cosine sim=%.3f: %s' % (cos[i], (idx2word[i])))\n",
    "        \n",
    "get_similar_tokens('云长', 10, net.in_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "190.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
