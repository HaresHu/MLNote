{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 综述\n",
    "\n",
    "![](assets/特征工程_xmind.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征选择\n",
    "\n",
    "L0，L1，L2优化\n",
    "\n",
    "`子集搜索与评价`\n",
    "\n",
    "1. 降低维度：维度灾难\n",
    "2. 去掉冗余特征，降低学习难度\n",
    "\n",
    "前向搜索与后向搜索，显然的贪心算法。\n",
    "\n",
    "如何评价：如信息增益\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 过滤式搜索\n",
    "\n",
    "Relief算法：对二分类\n",
    "\n",
    "Relief算法：对多分类\n",
    "\n",
    "\n",
    "\n",
    "构造统计分量，利用\"猜对近邻\"距离与“猜错近邻”距离小/大来增减统计分量值，最后筛选特征。\n",
    "$$\n",
    "\\delta^j=\\sum_{i}-diff(x_i^j,x_{i,nh}^{j})+diff(x_i^j,x_{i,nm}^{j})\n",
    "$$\n",
    "如果多分类：\n",
    "$$\n",
    "\\delta^j=\\sum_{i}-diff(x_i^j,x_{i,nh}^{j})+\\sum_{l \\neq k} p_l * diff(x_i^j,x_{i,l,nm}^{j})\n",
    "$$\n",
    "\n",
    "\n",
    "选择特征时还可以在样本中采样，不必在整个测试集。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 包裹式搜索\n",
    "\n",
    "LVW算法，如果采用拉斯维加斯框架，随机选择特征，训练，交叉验证给出错误率，如果特征更少，错误率更低，则选择\n",
    "\n",
    "显然，如果一开始选择特征很多，则开销很大，很可能有限时间无解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 嵌入式搜索\n",
    "\n",
    "正则化方法：特征很多，样本很少，很容易过拟合（函数空间很大，样本少的话，很容易就拟合了）\n",
    "\n",
    "参考书上图，解释为何L1比L2更有好处\n",
    "\n",
    "\n",
    "\n",
    "L1正则化优化方法：PGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  字典学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 压缩感知"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 离散化与交叉\n",
    "\n",
    "1. 直接用树模型找到切分点：相当于每次选择熵减少最大特征，然后按照排序\n",
    "2. 等频\n",
    "3. 等宽\n",
    "4. 直观\n",
    "\n",
    "\n",
    "\n",
    "GBDT+LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征排序\n",
    "\n",
    "如有特征x，取值100,10,9样本，数值型区分前者有点难，如果排序特征（类似与某种离散）\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考\n",
    "\n",
    "周志华《机器学习》\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/svm/plot_svm_anova.html#sphx-glr-auto-examples-svm-plot-svm-anova-py\n",
    "\n",
    "https://www.zhihu.com/question/28641663\n",
    "\n",
    "\n",
    "\n",
    "https://www.daimajiaoliu.com/daima/479c24f559003f0 onehote编码与使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
