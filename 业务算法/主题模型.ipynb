{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 综述\n",
    "pLSA:[Probabilistic latent semantic analysis](https://arxiv.org/pdf/1301.6705.pdf)，EM算法，较为简单，容易过拟合（M步存在参数：$p(z_k|d_m)$，根训练样本完全相关）\n",
    "\n",
    "LDA: 参考[lda-LDA数学八卦](http://bloglxm.oss-cn-beijing.aliyuncs.com/lda-LDA%E6%95%B0%E5%AD%A6%E5%85%AB%E5%8D%A6.pdf ):第45页很形象（与pLSA区别：topic属于一个Dirichlet分布），Gibbs Sample（推导用了一些近似方法）。短文本实际效果并不太好\n",
    "\n",
    "plsa泛化能力不好，主题分布是确定的，频率派参数估计基于已有数据。\n",
    "\n",
    "DA数学推理优秀，但是实际用的时候，gibbs sampling公式其实是一个近似正比，效果也没有非常理想。反而plsa简单容易并行化。\n",
    "\n",
    "\n",
    "# 工具\n",
    "gensim，百度也开源了LDA工具，LDA不像word2vec，需要去除停用词\n",
    "https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "\n",
    "# Ref\n",
    "[NLP —— 图模型（三）pLSA（Probabilistic latent semantic analysis，概率隐性语义分析）模型](https://www.cnblogs.com/Determined22/p/7237111.html)\n",
    "\n",
    "实践可以参考：https://www.cnblogs.com/chenbjin/p/5635853.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import gensim.models.ldamulticore\n",
    "from gensim.models import LdaModel,LdaMulticore\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "\n",
    "common_dictionary = Dictionary(common_texts)\n",
    "\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]\n",
    "\n",
    "# Train the model on the corpus.\n",
    "\n",
    "lda = LdaModel(common_corpus, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 速度更快\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "lda = LdaMulticore(common_corpus, id2word=common_dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.500*\"trees\" + 0.045*\"system\" + 0.045*\"response\" + 0.045*\"graph\" + 0.045*\"minors\" + 0.045*\"interface\" + 0.045*\"user\" + 0.045*\"human\" + 0.045*\"time\" + 0.045*\"survey\"'),\n",
       " (1,\n",
       "  '0.083*\"trees\" + 0.083*\"graph\" + 0.083*\"system\" + 0.083*\"user\" + 0.083*\"interface\" + 0.083*\"minors\" + 0.083*\"human\" + 0.083*\"time\" + 0.083*\"response\" + 0.083*\"computer\"'),\n",
       " (2,\n",
       "  '0.083*\"trees\" + 0.083*\"system\" + 0.083*\"graph\" + 0.083*\"user\" + 0.083*\"minors\" + 0.083*\"interface\" + 0.083*\"survey\" + 0.083*\"response\" + 0.083*\"human\" + 0.083*\"computer\"'),\n",
       " (3,\n",
       "  '0.083*\"trees\" + 0.083*\"graph\" + 0.083*\"system\" + 0.083*\"user\" + 0.083*\"interface\" + 0.083*\"time\" + 0.083*\"survey\" + 0.083*\"eps\" + 0.083*\"response\" + 0.083*\"human\"'),\n",
       " (4,\n",
       "  '0.262*\"minors\" + 0.262*\"graph\" + 0.262*\"survey\" + 0.024*\"trees\" + 0.024*\"system\" + 0.024*\"user\" + 0.024*\"interface\" + 0.024*\"human\" + 0.024*\"computer\" + 0.024*\"response\"'),\n",
       " (5,\n",
       "  '0.153*\"computer\" + 0.153*\"survey\" + 0.153*\"response\" + 0.153*\"system\" + 0.153*\"time\" + 0.153*\"user\" + 0.014*\"trees\" + 0.014*\"graph\" + 0.014*\"human\" + 0.014*\"interface\"'),\n",
       " (6,\n",
       "  '0.404*\"system\" + 0.212*\"human\" + 0.212*\"eps\" + 0.019*\"trees\" + 0.019*\"graph\" + 0.019*\"minors\" + 0.019*\"user\" + 0.019*\"interface\" + 0.019*\"time\" + 0.019*\"response\"'),\n",
       " (7,\n",
       "  '0.262*\"human\" + 0.262*\"computer\" + 0.262*\"interface\" + 0.024*\"trees\" + 0.024*\"graph\" + 0.024*\"user\" + 0.024*\"system\" + 0.024*\"minors\" + 0.024*\"response\" + 0.024*\"time\"'),\n",
       " (8,\n",
       "  '0.212*\"user\" + 0.212*\"eps\" + 0.212*\"system\" + 0.212*\"interface\" + 0.019*\"trees\" + 0.019*\"graph\" + 0.019*\"minors\" + 0.019*\"human\" + 0.019*\"time\" + 0.019*\"computer\"'),\n",
       " (9,\n",
       "  '0.228*\"trees\" + 0.228*\"graph\" + 0.120*\"user\" + 0.120*\"time\" + 0.120*\"minors\" + 0.120*\"response\" + 0.011*\"system\" + 0.011*\"interface\" + 0.011*\"survey\" + 0.011*\"computer\"')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"950\"\n",
       "            height=\"450\"\n",
       "            src=\"https://arxiv.org/pdf/1301.6705.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f71303ed710>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://arxiv.org/pdf/1301.6705.pdf', width=950, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Data and Co-occurrence Tables\n",
    "文档集合：$\\mathcal{D} = {d_1, d_2, ... , d_N}$\\\n",
    "构成文档的词汇表：$\\mathcal{W} = {w_1, w_2, ... , w_M}$\n",
    "\n",
    "term-document矩阵，也叫N×M共现计数表（忽略文档中词出现顺序）：$\\mathbf{N}=(n(d_i, w_j))_{ij}$, 其中$n(d,w)$表示文档d中w出现的频次。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Aspect Model\n",
    "![GraphicalMOdel](./assets/plsa_graphical_model.png)\n",
    "\n",
    "d,w共现概率：$P(d,w) = P(d)P(w|d)$\n",
    "\n",
    "引入潜在主题：$P(w|d) = \\sum_{z \\in Z }P(w|z)P(z|d)$\n",
    "\n",
    "tips:\n",
    "> **假设d与w独立**\n",
    "\n",
    "于是,根据(b),d,w的联合概率：\n",
    "$$\n",
    "P(d,w) = \\sum_{z \\in Z } P(z)P(d|z)P(w|z)\n",
    "$$\n",
    "根据最上面2个式子，也等于：\n",
    "$$\n",
    "P(d,w) =\\sum_{z \\in Z }P(w|z)P(z|d)  P(d)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model FItting with EM Algroithm\n",
    "\n",
    "tips:EM算法推导见：[聚类(k-means,EM与高斯混合)](https://localhost:8888/lab/workspaces/auto-v/tree/mm_note.git/0010-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/0040-%E8%81%9A%E7%B1%BB(k-means%2CEM%E4%B8%8E%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E8%81%9A%E7%B1%BB).ipynb)\n",
    "\n",
    "\n",
    "\n",
    "包括共现模型与生成模型。上述论文为共现模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立似然函数\n",
    "1） 语料生成概率：\n",
    "$$\n",
    "P(\\mathcal{D}) = \\prod_{i=1}^{N} \\prod_{j=1}^{M} (P(d_i, w_j))^{n(d_i,w_j)} \\ \\ \\ \\ (1)\n",
    "$$\n",
    "\n",
    "2) 似然函数\n",
    "$$\n",
    "{\\mathcal{L} = logP(\\mathcal{D}) = log \\prod_{i=1}^{N} \\prod_{j=1}^{M} (P(d_i, w_j))^{n(d_i,w_j)} \\\\\n",
    "= \\sum_{i=1}^{N} \\sum_{j=1}^{M} n(d_i, w_j)log P(d_i, w_j)\\\\\n",
    "= \\sum_{i=1}^{N} \\sum_{j=1}^{M} n(d_i, w_j) log( \\sum_{k=1}^{K} P(w_j|z_k)) P(z_k|d_i) P(d_i) \\ \\ \\ (2)\n",
    "}\n",
    "$$\n",
    "\n",
    "同时也等于：\n",
    "$$\n",
    "{\\mathcal{L} = \\sum_{i=1}^{N} \\sum_{j=1}^{M} n(d_i, w_j)log(\\sum_{k=1}^{K} P(w_j|z_k)) P(d_i|z_k) P(z_k) \\ \\ \\ (3)\n",
    "}\n",
    "$$\n",
    "\n",
    "对于（2）式子，可以进行如下拆分：\n",
    "$$\n",
    "{\\mathcal{L} = \\sum_{i=1}^{N} \\sum_{j=1}^{M} n(d_i, w_j) log( \\sum_{k=1}^{K} P(w_j|z_k)) P(z_k|d_i) P(d_i)\\\\\n",
    "= \\sum_{i=1}^{N} \\sum_{j=1}^{M} n(d_i, w_j)log(P(d_i)\\sum_{k=1}^{K} P(w_j|z_k)) P(z_k|d_i))\\\\\n",
    "= \\sum_{i=1}^{N} \\sum_{j=1}^{M} n(d_i, w_j)log(P(d_i)) + \\sum_{i=1}^{N} \\sum_{j=1}^{M} n(d_i, w_j)log(\\sum_{k=1}^{K} P(w_j|z_k)) P(z_k|d_i))\n",
    "}\n",
    "$$\n",
    "因为前半段是常量，可以去掉，所以:\n",
    "$$\n",
    "{\\mathcal{L} \\propto \\mathcal{{l}'} = \\sum_{i=1}^{N} \\sum_{j=1}^{M} n(d_i, w_j)log(\\sum_{k=1}^{K} P(w_j|z_k)) P(z_k|d_i)) \\\\ \n",
    "= \\sum_{i=1}^{N} \\sum_{j=1}^{M} n(d_i, w_j)log(p(w_j|d_i))\n",
    "} \\ \\ \\ \\ \\ \\ (4)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E Step:计算变量后验概率\n",
    "论文中是共现模型，可以对式（3）用Jensen不等式，得到Q函数\n",
    "$$\n",
    "P(z|d,w) = \\frac{P(z)P(d|z)P(w|z)}{\\sum_{{z}' \\in Z} P({z}')P(d|{z}')P(w|{z}')}\n",
    "$$\n",
    "\n",
    "**计算期望**：周志华《机器学习》-173页解释的简单清楚\n",
    "此时求：$E({l}')$,因为$P(z_k|d_i)$与$P(w_j|z_k)$为概率分布（相当于约束），则引入拉格朗日乘子，得到新的似然函数。\n",
    "\n",
    "1. 计算关于隐变量的后验概率（通过jensen不等式，得到Q函数就是基于隐变量的后验概率）\n",
    "2. 根据后验概率，可以的到基于${l}'$的期望\n",
    "\n",
    "此处，我们使用生成模型,根据jensen不等式，得到Q函数(也就是后验概率)：\n",
    "$$\n",
    "P(z_k|d_i, w_j) = \\frac{P(w_j|z_k)P(d_i|z_k)}{\\sum_{k=1}^{K}P(w_j|z_k)P(d_i|z_k)}\n",
    "$$\n",
    "\n",
    "此时初始化：$P(z_k|d_i)$与$P(w_j|z_k)$。通常要多初始化几组，防止陷入局部最优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M Step：更新参数\n",
    "极大化 $ \\mathcal{{l}’} $\n",
    "\n",
    "求导数，得到$P(z|d)$，$P(w|z)$公式，根据第一步结果，求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ref\n",
    "[PLSA详解推导](https://blog.csdn.net/ustccyf/article/details/8957082)\n",
    "\n",
    "\n",
    "https://my.oschina.net/u/3579120/blog/1508252 结合了论文，提到了$\\beta$，需要系看一下\n",
    "\n",
    "https://blog.csdn.net/hohaizx/article/details/88053429 推导也很详细\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/80557306 最清楚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
