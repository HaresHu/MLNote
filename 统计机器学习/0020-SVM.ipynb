{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 综述\n",
    "1） 线性svm（拉格朗日乘子法是多元函数在一组约束条件下求极值的方法，但是需要满足KKT条件，通俗易懂描述见周志华《机器学习》403页），优化方法SMO（每次更新2个$\\alpha$，因为其他已知）\n",
    "\n",
    "2） 基于核SVM（虽然很好的扩展到非线性，但是目前并没有好的选择核函数的方法），因为要两两计算距离，数据量很大时，效率很低。\n",
    "\n",
    "与LR有较大不一样地方，优化目标是最大化软间隔（从硬间隔到软间隔）\n",
    "\n",
    "损失函数：hinge loss\n",
    "\n",
    "拉格朗日对偶法：先将带约束的多元线性方程转换为带拉格朗日乘子的多项式，求导（要点是先消去b），再逐渐迭代\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 间隔与支持向量机\n",
    "\n",
    "![直观](./assets/v2-197913c461c1953c30b804b4a7eddfcc_720w.jpg)\n",
    "\n",
    "概念：最大化间隔，超平面划分,以及**支持向量**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 周志华版\n",
    "\n",
    "假设中间那条实线为划分超平面，$w^Tx + b$，根据点到直线距离（$\\frac{|w^Tx + b|}{||w||}$）(参考：[点到直线距离公式的几种推导](https://zhuanlan.zhihu.com/p/26307123))，在超平面取1点(x,y)分别到两条直线距离相加，得到两条直线距离为$\\frac{2}{||w||}$.\n",
    "\n",
    "如果超平面(w,b)能将训练样本正确分类，即对$(x_i,y_i)\\in D$，若$y_i=1$,则有$w^Tx + b > 0$;若$y_i=-1$,则有$w^Tx + b < 0$;令：\n",
    "$$\n",
    "\\left\\{\\begin{matrix}\n",
    "w^Tx + b \\geqslant +1,\\ y_i=+1\\\\ \n",
    "w^Tx + b \\leq  -1,\\ y_i=-1\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "目标是最大化间隔，\n",
    "\n",
    "$$\n",
    "{\\underset{w,b}{max} \\frac{2}{||w||} \\\\\n",
    "s.t.\\ y_i(w^Tx + b) \\geqslant  1, i=1,2,...,m.\n",
    "}\n",
    "$$\n",
    "\n",
    "转换为：\n",
    "\n",
    "$$\n",
    "{\\underset{w,b}{min} \\ \\frac{1}{2}{||w||}^2 \\\\\n",
    "s.t.\\ y_i(w^Tx + b) \\geqslant  1, i=1,2,...,m.\n",
    "\n",
    "}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  李航版本\n",
    "\n",
    "第一步找出所有点到分离超平面距离最小值$\\gamma $，而根据点在超平面正得一侧$w^Tx + b$大于0时，$y_i=1$,而小于0时，$y_i=-1$，可知，点到直线距离为$y_i \\cdot (\\frac{w^T}{||w||}\\textbf{x}_{i} + \\frac{b}{||w||})$\n",
    "\n",
    "问题就转换为：\n",
    "$$\n",
    "{\\underset{w,b}{max} \\ \\gamma \\\\\n",
    "s.t.\\ y_i \\cdot (\\frac{w^T}{||w||}\\textbf{x}_{i} + \\frac{b}{||w||}) \\geqslant  \\gamma, i=1,2,...,m.\n",
    "}\n",
    "$$\n",
    "考虑到函数间隔与几何间隔关系，去掉坟墓$||w||$,问题转换为：\n",
    "$$\n",
    "{\\underset{w,b}{max} \\ \\frac{\\hat{\\gamma}}{||w||} \\\\\n",
    "s.t.\\ y_i \\cdot (w^Tx + b) \\geqslant  \\hat{\\gamma}, i=1,2,...,m.\n",
    "}\n",
    "$$\n",
    "函数间隔$\\hat{\\gamma}$的值并不影响最优化问题的解，实际上，将w和b按比例改变为$\\lambda w$和$\\lambda b$，这时候函数间隔为$\\lambda \\cdot \\hat{\\gamma}$,对问题优化并无改变，所以，取$\\hat{\\gamma}=1$，问题转换为\n",
    "$$\n",
    "{\\underset{w,b}{min} \\ \\frac{1}{2}{||w||}^2 \\\\\n",
    "s.t.\\ y_i(w^Tx + b) \\geqslant  1, i=1,2,...,m.\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 证明解存在且唯一\n",
    "\n",
    "1. 证明存在：\n",
    "   核心就是数据本身线性可分，数据有正有负数，所以，代入可以知道$||w|| \\neq 0$，则存在\n",
    "\n",
    "2. 证明唯一：\n",
    "   反证法，假设不唯一，则存在2个解($(w_1,b_1), (w_2,b_2$)，第一步证明$w_1=w_2$，根据约束与最小值1得到：$||w_1|| = ||w_2||=c$（这是优化目标，必定相等）,于是$w=\\frac{w_1 + w_2}{2},b=\\frac{b_1 + b_2}{2}$也是方程解,则$c=||w|| \\leqslant \\frac{||w_1||}{2} + \\frac{||w_2||}{2}=c \\ \\Rightarrow \\ w_1=\\lambda w_2$，若$\\lambda=-1$,则$w=0$，所以$\\lambda=1$，即$w_1=w_2$，则2个解变为：$(w, b_1),(w,b_2)$\n",
    "\n",
    "   第二步证明$b_1=b_2$，假设${x}_1'$和${x}_2'$是集合$\\{x_i|y_i=+1\\}$中分别对应于$(w, b_1)$和$(w, b_2)$中使不等好成立的解（及$w \\cdot {x}'_1+b_1=1,w \\cdot {x}'_2+b_2=1$）,同时，${x}_1''$和${x}_2''$是集合$\\{x_i|y_i=-1\\}$中分别对应于$(w, b_1)$和$(w, b_2)$中使不等好成立的解（及$w \\cdot {x}''_1+b_1=-1,w \\cdot {x}''_2+b_2=-1$），有以上4个式子可得$b_1=-1 \\cdot \\frac{1}{2}(w \\cdot {x}'_1 + w \\cdot {x}''_1)$，$b_2=-1 \\cdot \\frac{1}{2}(w \\cdot {x}'_2 + w \\cdot {x}''_2)$，于是$b_1-b_2=-\\frac{1}{2}[w \\cdot ({x}'_1 - {x}'_2) + w \\cdot ({x}''_1 - {x}''_2)]$，既然存在2个不同超屏幕，则一个超平面对应的最优解在另一个超平面最少是最优解：\n",
    "   $$\n",
    "   {w \\cdot {x}'_2 + b_1 \\geqslant 1 = w \\cdot {x}'_1 + b_1 \\Rightarrow  w \\cdot ({x}'_1 - {x}'_2) \\leqslant 0\\\\\n",
    "   w \\cdot {x}'_1 + b_2 \\geqslant 1 = w \\cdot {x}'_2 + b_2  \\Rightarrow  w \\cdot ({x}'_1 - {x}'_2) \\Rightarrow 0 \\\\\n",
    "   \\Rightarrow w \\cdot ({x}'_1 - {x}'_2) = 0 \n",
    "   }\n",
    "   $$\n",
    "   同理$w \\cdot ({x}''_1 - {x}''_2) = 0$，因此$b_1 - b_2 = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 支持向量\n",
    "\n",
    "使得约束条件=0的点称之为支持向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 拉格朗日对偶法\n",
    "\n",
    "具体参见数学附录，此处仅明确，SVM中拉格朗日为强对偶条件,及满足KKT条件。\n",
    "\n",
    "问题转换为求(参考离航版):\n",
    "$$\n",
    "{\n",
    "\\underset{\\alpha}{max}\\ \\underset{w,b}{min} \\ L(w.b.\\alpha) = \\frac{1}{2}{||w||}^2 + \\sum_{i=1}^{m}{\\alpha}_i(1-y_i(w\\textbf{x}_i + b))\n",
    "} \n",
    "$$\n",
    "即先求$L(w,b,\\alpha)$对$w,b$的极小(即假设$\\alpha$已知)，再求对$\\alpha$的极大\n",
    "\n",
    "1) 第一步：求$\\underset{w,b}{min} \\ L(w.b.\\alpha)$\n",
    "\n",
    "分别对$w,b$求导数，令其等于0,并代入原式消去b，得到：\n",
    "$$\n",
    "\\underset{w,b}{min} \\ L(w.b.\\alpha) = -\\frac{1}{2}\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha_i\\alpha_jy_iy_j(x_i \\cdot x_j) + \\sum_{i=1}^{m}\\alpha_i\n",
    "$$\n",
    "\n",
    "2) 第二步：求$\\underset{w,b}{min} \\ L(w.b.\\alpha)$对$\\alpha$的极大，即是对偶问题：\n",
    "$$\n",
    "{\\underset{\\alpha}{max} \\  -\\frac{1}{2}\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha_i\\alpha_jy_iy_j(x_i \\cdot x_j) + \\sum_{i=1}^{m}\\alpha_i \\\\ \n",
    "s.t. \\ \\ {\\sum_{i=1}^{m}\\alpha_iy_i = 0 \\\\ \n",
    "\\alpha_i \\geqslant 0,i=1,2,...,m}\n",
    "}\n",
    "$$\n",
    "此时将目标函数乘以$-1$,问题由极大变极小\n",
    "$$\n",
    "{\\underset{\\alpha}{min} \\  \\frac{1}{2}\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha_i\\alpha_jy_iy_j(x_i \\cdot x_j) - \\sum_{i=1}^{m}\\alpha_i \\\\ \n",
    "s.t. \\ \\ {\\sum_{i=1}^{m}\\alpha_iy_i = 0 \\\\ \n",
    "\\alpha_i \\geqslant 0,i=1,2,...,m}\n",
    "}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 软间隔最大化与合页损失函数\n",
    "\n",
    "依然是线性可分，对每个样本点引入一个松弛变量，约束条件变为:\n",
    "$$\n",
    "y_i(w \\cdot x_i + b) \\geqslant 1-\\xi_i\n",
    "$$\n",
    "目标函数变为$\\frac{1}{2}||w|| + C\\sum_{i=1}^{m}\\xi_i$，其中C为惩罚参数，一般由应用问题确定，**C较大时，对误分类惩罚较大（因为目标是最小化）**此改进后，最小化目标函数就包含了2曾含义： 间隔尽可能大且误分类个数尽可能小。原始问题就变为：\n",
    "$$\n",
    "{\\underset{w,b,\\xi}{min} \\  \\frac{1}{2}||w||^2 + C\\sum_{i=1}^{m}\\xi_i\\\\\n",
    "s.t.\\ {y_i(w^Tx + b) \\geqslant  1-\\xi_i, i=1,2,...,m.\\\\\n",
    "\\xi_i \\geqslant 0, i=1,2,...,m}\n",
    "}\n",
    "$$\n",
    "\n",
    "同样采用拉格朗日对偶法，区别在于多了一个条件$0 \\leqslant \\alpha_i \\leqslant C $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合页损失函数\n",
    "\n",
    "令$\\lambda = \\frac{1}{2C}$，则目标函数变为\n",
    "$$\n",
    "\\underset{w,b,\\xi}{min} \\sum_{i=1}^{m}\\xi_i + \\lambda ||w|| = \\underset{w,b,\\xi}{min} \\sum_{i=1}^{m}[1 - y_i(w\\cdot x_i + b)]_+ + \\lambda ||w||^2\n",
    "$$\n",
    "令$[1 - y_i(w\\cdot x_i + b)]_+ = \\xi_i$，当$1 - y_i(w\\cdot x_i + b) > 0$时，说明是误分类点，$y_i(w\\cdot x_i + b) = 1 - \\xi_i$，当$1 - y_i(w\\cdot x_i + b) \\leqslant 0$，即正确分类，$\\xi_i=0$,此时$y_i(w\\cdot x_i + b) = 1 - \\xi_i$所以，**目标函数可以转变为上式**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数比较\n",
    "\n",
    "![img](https://img-blog.csdnimg.cn/20200329213906998.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoZW56aWhlbmcx,size_16,color_FFFFFF,t_70)\n",
    "\n",
    "x轴：函数间隔，即$y(w\\cdot x+b)$,y轴是损失\n",
    "\n",
    "还差一个感知机器，x轴小于0,则是个一次函数(如$y=-x$)\n",
    "\n",
    "这样就很容易理解，如果$y \\cdot (w \\cdot x + b)$ 在不同算法下，正负样本阈值不同，如合页损失函数阈值为1\n",
    "\n",
    "还包括:对率损失函数$log(1 + exp(-z))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 核函数与非线性SVM\n",
    "\n",
    "能够做核的，必须满足：\n",
    "\n",
    "令$\\chi$为输入空间，$\\kappa(\\cdot , \\cdot)$是定义在$\\chi × \\chi$上的对称函数，则$\\kappa$是核函数，当且仅当对于任意数据$D = {x_1, x_2,...,x_m}$，“核矩阵”（kernel matrix）$K$总是半正定的\n",
    "$$\n",
    "K =\n",
    "\\begin{bmatrix}\n",
    "\\kappa(x_1,x_1) & ... & \\kappa(x_1,x_j) & ... & \\kappa(x_1,x_m)\\\\ \n",
    " \\vdots & \\ddots  & \\vdots  & \\ddots  & \\vdots \\\\ \n",
    "\\kappa(x_i,x_1) & ... & \\kappa(x_i,x_j) & ... & \\kappa(x_i,x_m)\\\\\n",
    " \\vdots & \\ddots  & \\vdots  & \\ddots  & \\vdots \\\\ \n",
    "\\kappa(x_m,x_1) & ... & \\kappa(x_m,x_j) & ... & \\kappa(x_m,x_m)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "常见核函数：\n",
    "\n",
    "![img](./assets/8140224-44baf2fa35f4e2b6.png)\n",
    "\n",
    "\n",
    "\n",
    "参考[SVM中，高斯核为什么会把原始维度映射到无穷多维？](https://www.zhihu.com/question/35602879)\n",
    "\n",
    "数据量很大的时候，核方法效率很低，需要计算距离。\n",
    "\n",
    "\n",
    "\n",
    "训练时候可以使用之前的线性训练方法。预测函数:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMO算法\n",
    "\n",
    "启发式算法，假设所有变量都满足此最优解的KKT条件（KKT条件是该问题的充分必要条件）。\n",
    "\n",
    "从约束可知，每次更新两个$\\alpha$，固定其他$\\alpha$，而根据约束$\\sum_{i=1}^{m}\\alpha_iy_i = 0$，知道其中一个，就能解另外一个。\n",
    "\n",
    "也就是转变为单变量二次规划问题\n",
    "\n",
    "参考：https://zhuanlan.zhihu.com/p/78599113，与李航书保持一致\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 支持向量回归\n",
    "\n",
    "核心是，给出一个$\\xi$，损失函数$\\Xi$不敏感函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 其他\n",
    "\n",
    "训练数据不宜过大。（计算样本间高斯核距离）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
