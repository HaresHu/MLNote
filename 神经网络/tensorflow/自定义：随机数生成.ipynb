{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 综述\n",
    "TensorFlow 提供了两种方法来控制随机数生成过程：\n",
    "\n",
    "- 通过明确使用 tf.random.Generator 对象。每个此类对象都会在 tf.Variable 中维护一个状态，该状态在每次生成随机数后都会发生改变。\n",
    "\n",
    "- 通过使用纯函数式无状态随机函数，如 tf.random.stateless_uniform。在同一设备上调用具有相同参数（包括种子）的这些函数会产生相同的结果。\n",
    "\n",
    "> 警告：目前尚未弃用 TF 1.x 中的旧版 RNG（如 tf.random.uniform 和 tf.random.normal），但强烈建议不要使用。\n",
    "> \n",
    "> 警告：不保证随机数在不同 TensorFlow 版本间一致，请参阅：版本兼容性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Creates 2 virtual devices cpu:0 and cpu:1 for using distribution strategy\n",
    "physical_devices = tf.config.experimental.list_physical_devices(\"CPU\")\n",
    "tf.config.experimental.set_virtual_device_configuration(\n",
    "    physical_devices[0], [\n",
    "        tf.config.experimental.VirtualDeviceConfiguration(),\n",
    "        tf.config.experimental.VirtualDeviceConfiguration()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.random.Generator 类\n",
    "要点：\n",
    "- 每次调用结果不一样\n",
    "- 通过tf.Variable维护一个内部状态（因为是Variable变量，可以被checkpoint，自动控制依赖项，线程安全）\n",
    "\n",
    "如何创建：\n",
    "- 通过手动创建 tf.random.Generator类的一个对象，可以获得该生成器\n",
    "- 调用 tf.random.get_global_generator()，您可以获得默认全局生成器：\n",
    "\n",
    "\n",
    "# 创建方法1：tf.random.Generator\n",
    "tf.random.Generator.from_seed,种子是任何非负整数，alg表示使用的RNG算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.43842274 -0.53439844 -0.07710262]\n",
      " [ 1.5658046  -0.1012345  -0.2744976 ]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "g1 = tf.random.Generator.from_seed(1, alg='philox')\n",
    "print(g1.normal(shape=[2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator.from_non_deterministic_state：生成器首先会处于非确定状态，具体取决于时间和操作系统等因素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-1.0283433   0.5518764   1.2624873 ]\n",
      " [ 0.98823726  0.32675055  1.5768538 ]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "g = tf.random.Generator.from_non_deterministic_state()\n",
    "print(g.normal(shape=[2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有其他方法可以创建生成器，比如说通过显式状态创建@TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建方法2：tf.random.get_global_generator\n",
    "\n",
    "获取全局生成器。\n",
    "`第一次调用时会从非确定状态创建全局生成器，并放置在调用处的作用域的默认设备上`\n",
    "\n",
    "比如说在GPU上创建，如果稍后在CPU使用，则会从GPU复制到CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1.8368195  -0.49554613  1.2101773 ]\n",
      " [ 1.7602633   0.7141847  -0.62642   ]], shape=(2, 3), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.context._EagerDeviceContext at 0x7f4dd82ca490>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g2 = tf.random.get_global_generator()\n",
    "print(g2.normal(shape=[2, 3]))\n",
    "tf.device(\"gpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建独立的随机数流\n",
    "多个应用需要独立的随机数时，不想跨设备复制随机数生成器都要求能创建独立的随机数流。\n",
    "\n",
    "通过使用 Generator.split 创建多个一定相互独立的生成器即可;与 normal 之类的 RNG 方法类似，split 会改变调用它的生成器的状态（上例中为 g）。除相互之间保持独立外，新生成器 (new_gs) 还一定独立于旧生成器 (g)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.43842274, shape=(), dtype=float32)\n",
      "tf.Tensor(2.536413, shape=(), dtype=float32)\n",
      "tf.Tensor(0.33186463, shape=(), dtype=float32)\n",
      "tf.Tensor(-0.07144657, shape=(), dtype=float32)\n",
      "tf.Tensor(-0.79253083, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "g = tf.random.Generator.from_seed(1)\n",
    "print(g.normal([]))\n",
    "new_gs = g.split(3)\n",
    "for new_g in new_gs:\n",
    "  print(new_g.normal([]))\n",
    "print(g.normal([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0648588, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"cpu\"):  # change \"cpu\" to the device you want\n",
    "  g = tf.random.get_global_generator().split(1)[0]  \n",
    "  print(g.normal([]))  # use of g won't cause cross-device copy, unlike the global generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：在理论上，此处可以使用 from_seed（而不是 split）之类的构造函数获取新生成器，但这样做无法保证新生成器与全局生成器相互独立。同时也有使用同一种子或导致产生重叠随机数流的种子意外创建两个生成器的风险。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 与tf.function交互\n",
    "与 tf.function 一起使用时，tf.random.Generator 遵循与 tf.Variable 相同的原则。这包括三个方面：\n",
    "1. 在外部创建：调用该函数时，用户需要确保生成器对象仍处于活动状态（没有被回收）\n",
    "2. 在内部创建：只有 tf.function 第一次运行时，才可以在其内部创建生成器\n",
    "3. 作为参数传递：具有不同状态大小的不同生成器对象则会导致回溯。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.43842274, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 外部\n",
    "g = tf.random.Generator.from_seed(1)\n",
    "@tf.function\n",
    "def foo():\n",
    "  return g.normal([])\n",
    "print(foo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.43842274, shape=(), dtype=float32)\n",
      "tf.Tensor(1.6272374, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 内部\n",
    "g = None\n",
    "@tf.function\n",
    "def foo():\n",
    "  global g\n",
    "  if g is None:\n",
    "    g = tf.random.Generator.from_seed(1)\n",
    "  return g.normal([])\n",
    "print(foo())\n",
    "print(foo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# 传参\n",
    "num_traces = 0\n",
    "@tf.function\n",
    "def foo(g):\n",
    "  global num_traces\n",
    "  num_traces += 1\n",
    "  return g.normal([])\n",
    "foo(tf.random.Generator.from_seed(1))\n",
    "foo(tf.random.Generator.from_seed(2))\n",
    "print(num_traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 与分布策略交互\n",
    "也包括三种：\n",
    "1. 在分布策略的外部创建生成器，会被序列化访问此生成器不同副本，没一个都是不同的随机数\n",
    "\n",
    "> 会有性能问题，因为生成器的设备与副本不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0', '/job:localhost/replica:0/task:0/device:CPU:1')\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n",
      "tf.Tensor(0.43842274, shape=(), dtype=float32)\n",
      "tf.Tensor(1.6272374, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "g = tf.random.Generator.from_seed(1)\n",
    "strat = tf.distribute.MirroredStrategy(devices=[\"cpu:0\", \"cpu:1\"])\n",
    "with strat.scope():\n",
    "  def f():\n",
    "    print(g.normal([]))\n",
    "  results = strat.run(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 在策略内部创建生成器\n",
    "\n",
    "> 不允许在策略作用域内部创建生成器，因为这会导致在如何复制生成器方面出现歧义。比方说，是应该复制生成器，从而让每一个副本都获得相同的随机数，还是应该“拆分”，从而让每一个副本获得不同的随机数。\n",
    "\n",
    "\n",
    "ps:请注意，Strategy.run 会在策略作用域内隐式运行参数函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0', '/job:localhost/replica:0/task:0/device:CPU:1')\n",
      "ValueError: Creating a generator within a strategy scope is disallowed, because there is ambiguity on how to replicate a generator (e.g. should it be copied so that each replica gets the same random numbers, or 'split' so that each replica gets different random numbers).\n"
     ]
    }
   ],
   "source": [
    "strat = tf.distribute.MirroredStrategy(devices=[\"cpu:0\", \"cpu:1\"])\n",
    "with strat.scope():\n",
    "  try:\n",
    "    tf.random.Generator.from_seed(1)\n",
    "  except ValueError as e:\n",
    "    print(\"ValueError:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0', '/job:localhost/replica:0/task:0/device:CPU:1')\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n",
      "INFO:tensorflow:Error reported to Coordinator: Creating a generator within a strategy scope is disallowed, because there is ambiguity on how to replicate a generator (e.g. should it be copied so that each replica gets the same random numbers, or 'split' so that each replica gets different random numbers).\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hdw/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\n",
      "    yield\n",
      "  File \"/home/hdw/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_run.py\", line 323, in run\n",
      "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
      "  File \"/home/hdw/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"<ipython-input-18-2cd7806456bd>\", line 3, in f\n",
      "    tf.random.Generator.from_seed(1)\n",
      "  File \"/home/hdw/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/stateful_random_ops.py\", line 441, in from_seed\n",
      "    return cls(state=state, alg=alg)\n",
      "  File \"/home/hdw/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/stateful_random_ops.py\", line 363, in __init__\n",
      "    trainable=False)\n",
      "  File \"/home/hdw/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/stateful_random_ops.py\", line 378, in _create_variable\n",
      "    \"Creating a generator within a strategy scope is disallowed, because \"\n",
      "ValueError: Creating a generator within a strategy scope is disallowed, because there is ambiguity on how to replicate a generator (e.g. should it be copied so that each replica gets the same random numbers, or 'split' so that each replica gets different random numbers).\n",
      "ValueError: Creating a generator within a strategy scope is disallowed, because there is ambiguity on how to replicate a generator (e.g. should it be copied so that each replica gets the same random numbers, or 'split' so that each replica gets different random numbers).\n"
     ]
    }
   ],
   "source": [
    "strat = tf.distribute.MirroredStrategy(devices=[\"cpu:0\", \"cpu:1\"])\n",
    "def f():\n",
    "  tf.random.Generator.from_seed(1)\n",
    "try:\n",
    "  strat.run(f)\n",
    "except ValueError as e:\n",
    "  print(\"ValueError:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 将生成器作为参数传递给 Strategy.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0', '/job:localhost/replica:0/task:0/device:CPU:1')\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n",
      "tf.Tensor(1.3995249, shape=(), dtype=float32)\n",
      "tf.Tensor(-0.18214026, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "strat = tf.distribute.MirroredStrategy(devices=[\"cpu:0\", \"cpu:1\"])\n",
    "gs = tf.random.get_global_generator().split(2)\n",
    "# to_args is a workaround for the absence of APIs to create arguments for \n",
    "# run. It will be replaced when such APIs are available.\n",
    "def to_args(gs):  \n",
    "  with strat.scope():\n",
    "    def f():\n",
    "      return [gs[tf.distribute.get_replica_context().replica_id_in_sync_group]]\n",
    "    return strat.run(f)\n",
    "args = to_args(gs)\n",
    "def f(g):\n",
    "  print(g.normal([]))\n",
    "results = strat.run(f, args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 无状态RNG\n",
    "\n",
    "无状态 RNG 的使用方法非常简单。因为它们是纯函数，不涉及状态或副作用。\n",
    "\n",
    "> 但是相同的seed，会导致相同结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.5441101   0.20738031  0.07356433]\n",
      " [ 0.04643455 -1.3015898  -0.95385665]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.5441101   0.20738031  0.07356433]\n",
      " [ 0.04643455 -1.3015898  -0.95385665]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.random.stateless_normal(shape=[2, 3], seed=[1, 2]))\n",
    "print(tf.random.stateless_normal(shape=[2, 3], seed=[1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 算法\n",
    "philox\n",
    "\n",
    "XLA设备还包括：threefry\n",
    "\n",
    "@TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
